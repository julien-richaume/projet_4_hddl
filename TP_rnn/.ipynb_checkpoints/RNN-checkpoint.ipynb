{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "The objective of this lab project is to train a RNN from scratch, by performing all the necessary calculations and deriving the formulas for the BPTT algorithm. We will code a RNN from scratch using numpy. The network will perform the task of \"next character generation\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Library imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Coding a RNN from scratch\n",
    "In this section we will code a RNN from scratch using numpy. The network will perform the task of \"next character generation\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Data\n",
    "In order to train to you RNN, go to [Project Gutemberg](https://www.gutenberg.org/) and choose a book that you like. Copy paste it as plain text in a text file called `book.txt` in your current repository.\n",
    "If you struggle to choose a book, here are a few options :\n",
    "- [Les misérables](https://www.gutenberg.org/cache/epub/48735/pg48735.txt)\n",
    "- [Don Quijote](https://www.gutenberg.org/cache/epub/2000/pg2000.txt)\n",
    "- [La Divina Commedia](https://www.gutenberg.org/cache/epub/997/pg997.txt)\n",
    "- [L'avare](https://www.gutenberg.org/cache/epub/6318/pg6318.txt)\n",
    "\n",
    "**Exercise.** Once the book of your choice is copied onto `book.txt` write a `load_data` function that:\n",
    "1. Reads the file `book.txt` and returns it as a string.\n",
    "2. Returns a sorted list containing all unique characters in the book.\n",
    "3. Returns the number of unique characters.\n",
    "4. Returns a dictionary mapping the characters in the alphabet to unique numerical indexes.\n",
    "5. Returns a dictionary to perform the reverse mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset\n",
    "def load_data(file_path):\n",
    "    #TODO: Load the data from the file and store it in the variable text\n",
    "    with open(file_path, 'r', encoding='utf8') as f :\n",
    "        text = f.read()\n",
    "    chars = sorted(set(text)) #TODO: Find the unique characters in the text\n",
    "    vocab_size = len(chars) #TODO: Find the number of unique characters in the text\n",
    "    char_to_ix = {ch : i for i, ch in enumerate(chars)} #TODO: Create a dictionary that maps characters to indices\n",
    "    ix_to_char = {i : ch for i, ch in enumerate(chars)} #TODO: Create a dictionary that maps indices to characters\n",
    "    return text, chars, vocab_size, char_to_ix, ix_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/load_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book has 174809 characters and 107 unique characters\n"
     ]
    }
   ],
   "source": [
    "text, chars, vocab_size, char_to_ix, ix_to_char = load_data('book.txt')\n",
    "print(f\"The book has {len(text)} characters and {vocab_size} unique characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " qu'indifférent.\n",
      "\n",
      "  Son pareil le suivait: barbe, oeil, dos, bâton, loques,\n",
      "  Nul trait ne distinguait, du même enfer venu,\n",
      "  Ce jumeau centenaire, et ces spectres baroques\n",
      "  Marchaient du même pas vers un but inconnu.\n",
      "\n",
      "  A quel complot infâme étais-je donc en butte,\n",
      "  Ou quel méchant hasard ainsi m'humiliait?\n",
      "  Car je comptai sept fois, de minute en minute,\n",
      "  Ce sinistre vieillard qui se multipliait!\n",
      "\n",
      "  Que celui-là qui rit de mon inquiétude,\n",
      "  Et qui n'est pas saisi d'un frisson fraternel\n",
      "  So\n"
     ]
    }
   ],
   "source": [
    "# An excerpt from the book\n",
    "print(text[100000:100500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. The RNN architecture\n",
    "We will build a simple RNN architecture obeying to the following formulas:\n",
    "\\begin{align*}\n",
    "a_t &= \\tanh(W_{aa}a_{t-1} + W_{ax}x_t + b_a)\\\\\n",
    "\\hat{y}_t &= W_{ya}a_t + b_y\\\\\n",
    "\\hat{o}_t &= \\text{softmax}(\\hat{y}_t)\n",
    "\\end{align*}\n",
    "In order to do so, we will define a class called RNN. The class RNN will consist of several different methods:\n",
    "- An `__init__` method, where the different weight matrices and bias vectors should be initialized as follows:\n",
    "    - The vocabulary size, and hidden state size should be passed as arguments on initialization and stored for future use.\n",
    "    - The weight matrices should be initialized to have i.i.d. entries with distribution $\\mathcal{N}(0, 0.001)$.\n",
    "    - The bias vectors should be initialized to 0.\n",
    "    - In addition we will initialize parameters necessary for the AdaGrad optimization algorith\n",
    "- A `forward` method, where the above formulas should be implemented.\n",
    "- A `compute_gradients` method, where the formulas for backpropagation through time should be implemented and the gradients of the different weight matrices and bias vectors computed.\n",
    "- A `backward_adagrad` method that will update the values of the different weight matrices and bias vectors using the gradients values and the AdaGrad algorithm.\n",
    "- A `sample` method to sample a new sentence of a given length, by providing only the first letter of the sentence.\n",
    "\n",
    "In the sequel we will implement these five methods step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. The `__init__` method\n",
    "**Exercise.** Complete the `__init__` method below by filling in the `TODO` flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "\n",
    "    def __init__(self, vocab_size, hidden_size=64):\n",
    "        # TODO: store the model parameters in variables of the same name\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # TODO: initialize the model weights and biases\n",
    "        self.Waa = np.random.randn(hidden_size, hidden_size) * 1e-3\n",
    "        self.Wax = np.random.randn(hidden_size, vocab_size) * 1e-3\n",
    "        self.Wya = np.random.randn(vocab_size, hidden_size) * 1e-3\n",
    "        self.ba = np.random.randn(hidden_size) * 1e-3\n",
    "        self.by = np.random.randn(vocab_size) * 1e-3\n",
    "\n",
    "        # AdaGrad optimization parameters\n",
    "        self.m = {k: np.zeros_like(v) for k, v in self.__dict__.items() if isinstance(v, np.ndarray)}\n",
    "\n",
    "    \n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def compute_gradients(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def backward_adagrad(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def sample(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. The `forward` method\n",
    "**Exercise.** Complete the `forward` method by filling in the `TODO` flags below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    # Simple \"vanilla\" RNN model\n",
    "\n",
    "    def __init__(self, vocab_size, hidden_size=64):\n",
    "        # Store the model hyperparameters\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Initialize the model weights and biases\n",
    "        self.Waa = np.random.randn(hidden_size, hidden_size) / 1000\n",
    "        self.Wax = np.random.randn(hidden_size, vocab_size) / 1000\n",
    "        self.Wya = np.random.randn(vocab_size, hidden_size) / 1000\n",
    "        self.ba = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((vocab_size, 1))\n",
    "        \n",
    "        # AdaGrad optimization parameters\n",
    "        self.m = {k: np.zeros_like(v) for k, v in self.__dict__.items() if isinstance(v, np.ndarray)}\n",
    "        \n",
    "\n",
    "    def forward(self, inputs, h_prev):\n",
    "        \"\"\"\n",
    "        Perform forward pass.\n",
    "        Args:\n",
    "            inputs: One-hot encoded inputs of shape (vocab_size, seq_length)\n",
    "            h_prev: Initial hidden state of shape (hidden_size, 1)\n",
    "        Returns:\n",
    "            outputs: List of output probabilities\n",
    "            hs: List of hidden states\n",
    "        \"\"\"\n",
    "        # Initialize hidden state, logits, and output pobabilities\n",
    "        hs, ys, ps = {}, {}, {}\n",
    "        hs[-1] = np.copy(h_prev)\n",
    "\n",
    "        for t, x in enumerate(inputs):\n",
    "            # TODO: update the hidden state\n",
    "            hs[t] = np.tanh(self.Waa @ h_prev[t-1] + self.Wax @ x + self.ba)\n",
    "\n",
    "            # TODO: compute the output logits\n",
    "            ys[t] = self.Wya @ hs[t] + self.by\n",
    "\n",
    "            # TODO: compute the softmax probabilities\n",
    "            ps[t] = np.exp(ys[t])/np.sum(np.exp(ys[t]))\n",
    "\n",
    "        # We return the whole history of output probabilities and hidden states \n",
    "        # since we will need them for the backpropagation\n",
    "        return ps, hs\n",
    "    \n",
    "\n",
    "    def compute_gradients(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def backward_adagrad(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def sample(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3. The `compute_gradients` method\n",
    "In order to train our RNN we need to implement the BPTT algorith. We will use the cross-entropy loss, which is the common practice associated with the softmax function. For an output sequence of length $T$, let us denote by $(y_t)_{t=1}^T$ the sequence of ground truth index values, and let us denote by $(\\hat{y}_t)_{t=1}^T$ the sequence outputed by the RNN (i.e. the vector of subsequent logit values).\n",
    "We wish to optimize the loss function\n",
    "$$\\mathcal{L}=\\sum_{t=1}^T L_t$$\n",
    "where each term $L_t$ is given by the negative log-likelihood of the ground truth output under the current model:\n",
    "$$\\displaystyle L_t = -\\log \\hat{o}_t(y_t) = -\\log \\text{softmax}_{y_t}(\\hat{y}_t).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to ease our calculations, we introduce the following notation:\n",
    "\\begin{align*}\n",
    "dW_{ax} := \\frac{\\partial\\mathcal{L}}{\\partial W_{ax}}\n",
    "dW_{aa} := \\frac{\\partial\\mathcal{L}}{\\partial W_{aa}}\n",
    "dW_{ya} := \\frac{\\partial\\mathcal{L}}{\\partial W_{ya}}\n",
    "db_{a} := \\frac{\\partial\\mathcal{L}}{\\partial b_{a}}\n",
    "db_{y} := \\frac{\\partial\\mathcal{L}}{\\partial b_{y}}\n",
    "\\end{align*}\n",
    "The objective of the `compute_gradients` methods is to compute the above quantities. We will do so iteratively by traveling backward over the unfolded RNN graph, here is the pseude-code to implement the `compute_gradients` method:\n",
    "\n",
    "For $t = T, T-1, \\dots, 1$:\n",
    "- compute $dy_t:=\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_t}$.\n",
    "- update $dW_{ya}$ and $db_a$.\n",
    "- backpropagate the gradient to the hidden layer, i.e. compute \n",
    "$dh_t := \\frac{\\partial \\mathcal{L}}{\\partial h_t}$.\n",
    "\n",
    "\n",
    "- update $dW_{aa}$, $dW_{xa}$ and $db_a$ using the chain rule: for example, for $dW_{aa}$\n",
    "$$dW_{aa} := \\frac{\\partial\\mathcal{L}}{\\partial W_{aa}}= \\sum_{t=1}^T \\frac{\\partial h_t}{\\partial W_{aa}} \\frac{\\partial\\mathcal{L}}{\\partial h_t}.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Compute the necessary quantities to deploy the pseudo-code above:\n",
    "1. Find a formula for $dy_t$.\n",
    "2. Find a formula to compute $dW_{ya}$ and $db_a$ as a function of the $dy_1,\\dots,dy_T$.\n",
    "3. Find a formula to compute $dh_t$ as a function of $dy_t$ and $dh_{t+1}$.\n",
    "4. Find formulas to compute $dW_{aa}$, $dW_{ax}$ and $db_a$ as functions of $dh_1,\\dots,dh_T$.\n",
    "\n",
    "**Use pen and paper! Ask the teachers if you struggle!** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:teal \">[Solution]</span>\n",
    "\n",
    " **Solution**:\n",
    "* $dy_t = \\hat{o}_t - y_t$\n",
    "* $dW_{ya} = \\sum_{t=1}^T dy_t h_t^\\top$\n",
    "* $db_y = \\sum_{t=1}^T dy_t$\n",
    "* $dh_t= W_{ya}^\\top dy_t + W_{aa}^\\top (1 - h_{t+1}^2) * dh_{t + 1}$\n",
    "* $dW_{aa} = \\sum_{t=1}^T (1- h_t^2) * dh_t h_t^\\top$\n",
    "* $dW_{ax} = \\sum_{t=1}^T (1- h_t^2) * dh_t x_t^\\top$\n",
    "* $db_a = \\sum_{t=1}^T (1- h_t^2) * dh_t$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Complete the `compute_gradients` methods by filling in the `TODO` flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    # Simple \"vanilla\" RNN model\n",
    "\n",
    "    def __init__(self, vocab_size, hidden_size=64):\n",
    "        # Store the model hyperparameters\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Initialize the model weights and biases\n",
    "        self.Waa = np.random.randn(hidden_size, hidden_size) / 1000\n",
    "        self.Wax = np.random.randn(hidden_size, vocab_size) / 1000\n",
    "        self.Wya = np.random.randn(vocab_size, hidden_size) / 1000\n",
    "        self.ba = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((vocab_size, 1))\n",
    "        \n",
    "        # AdaGrad optimization parameters\n",
    "        self.m = {k: np.zeros_like(v) for k, v in self.__dict__.items() if isinstance(v, np.ndarray)}\n",
    "\n",
    "\n",
    "    def forward(self, inputs, h_prev):\n",
    "        \"\"\"\n",
    "        Perform forward pass.\n",
    "        Args:\n",
    "            inputs: One-hot encoded inputs of shape (vocab_size, seq_length)\n",
    "            h_prev: Initial hidden state of shape (hidden_size, 1)\n",
    "        Returns:\n",
    "            outputs: List of output probabilities\n",
    "            hs: List of hidden states\n",
    "        \"\"\"\n",
    "        # Initialize hidden state, logits, and output pobabilities\n",
    "        hs, ys, ps = {}, {}, {}\n",
    "        hs[-1] = np.copy(h_prev)\n",
    "\n",
    "        for t, x in enumerate(inputs):\n",
    "            # Update the hidden state\n",
    "            hs[t] = np.tanh(np.dot(self.Wax, x) + np.dot(self.Waa, hs[t-1]) + self.ba)\n",
    "\n",
    "            # Compute the output logits\n",
    "            ys[t] = np.dot(self.Wya, hs[t]) + self.by\n",
    "\n",
    "            # Softmax probabilities\n",
    "            ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
    "\n",
    "        return ps, hs\n",
    "\n",
    "\n",
    "    def compute_gradients(self, inputs, targets, hs, ps):\n",
    "        \"\"\"\n",
    "        Compute the gradients for one sequence.\n",
    "        Args:\n",
    "            inputs: List of one-hot encoded inputs\n",
    "            targets: List of integer targets\n",
    "            hs: List of hidden states\n",
    "            ps: List of output probabilities\n",
    "        Returns:\n",
    "            Gradients\n",
    "        \"\"\"\n",
    "        #TODO: Initialize gradients to zero matrices/vectors of the appropriate shapes\n",
    "        dWax, dWaa, dWya = np.zeros_like(self.Wax), np.zeros_like(self.Waa), np.zeros_like(self.Wya)\n",
    "        dba, dby = np.zeros_like(self.ba), np.zeros_like(self.by)\n",
    "\n",
    "        #TODO: Initilized the hidden state gradient to a zero vector of the appropriate size\n",
    "        dh_next = np.zeros_like(hs)\n",
    "\n",
    "        # Loop through each time step\n",
    "        for t in reversed(range(len(targets))):\n",
    "            # TODO: Compute dy, the derivative of the loss with respect to the output\n",
    "            dy = ...\n",
    "\n",
    "            #TODO: Update the gradients of the output layer\n",
    "            dWya += ...\n",
    "            dby += ...\n",
    "\n",
    "            #TODO: Backpropagate the gradient to the hidden layer\n",
    "            dh = ...\n",
    "            dh_raw = (1 - hs[t] ** 2) * dh # Backprop through tanh\n",
    "\n",
    "            # TODO: Update the gradients of the hidden layer\n",
    "            dWax += ...\n",
    "            dWaa += ...\n",
    "            dba += ...\n",
    "\n",
    "            # Update the next hidden state\n",
    "            dh_next = ...\n",
    "\n",
    "        # Clip gradients to mitigate exploding gradients\n",
    "        for dparam in [dWax, dWaa, dWya, dba, dby]:\n",
    "            np.clip(dparam, -1, 1, out=dparam)\n",
    "        \n",
    "        return dWax, dWaa, dWya, dba, dby\n",
    "        \n",
    "\n",
    "\n",
    "    def backward_adagrad(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def sample(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4. The `backward_adagrad` method\n",
    "Below we add the adagrad optimization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    # Simple \"vanilla\" RNN model\n",
    "\n",
    "    def __init__(self, vocab_size, hidden_size=64):\n",
    "        # Store the model hyperparameters\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Initialize the model weights and biases\n",
    "        self.Waa = np.random.randn(hidden_size, hidden_size) / 1000\n",
    "        self.Wax = np.random.randn(hidden_size, vocab_size) / 1000\n",
    "        self.Wya = np.random.randn(vocab_size, hidden_size) / 1000\n",
    "        self.ba = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((vocab_size, 1))\n",
    "        \n",
    "        # AdaGrad optimization parameters\n",
    "        self.m = {k: np.zeros_like(v) for k, v in self.__dict__.items() if isinstance(v, np.ndarray)}\n",
    "        \n",
    "\n",
    "    def forward(self, inputs, h_prev):\n",
    "        \"\"\"\n",
    "        Perform forward pass.\n",
    "        Args:\n",
    "            inputs: One-hot encoded inputs of shape (vocab_size, seq_length)\n",
    "            h_prev: Initial hidden state of shape (hidden_size, 1)\n",
    "        Returns:\n",
    "            outputs: List of output probabilities\n",
    "            hs: List of hidden states\n",
    "        \"\"\"\n",
    "        # Initialize hidden state, logits, and output pobabilities\n",
    "        hs, ys, ps = {}, {}, {}\n",
    "        hs[-1] = np.copy(h_prev)\n",
    "\n",
    "        for t, x in enumerate(inputs):\n",
    "            # Update the hidden state\n",
    "            hs[t] = np.tanh(np.dot(self.Wax, x) + np.dot(self.Waa, hs[t-1]) + self.ba)\n",
    "\n",
    "            # Compute the output logits\n",
    "            ys[t] = np.dot(self.Wya, hs[t]) + self.by\n",
    "\n",
    "            # Softmax probabilities\n",
    "            ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
    "\n",
    "        return ps, hs\n",
    "\n",
    "    def compute_gradients(self, inputs, targets, hs, ps):\n",
    "        \"\"\"\n",
    "        Compute the gradients for one sequence.\n",
    "        Args:\n",
    "            inputs: List of one-hot encoded inputs\n",
    "            targets: List of integer targets\n",
    "            hs: List of hidden states\n",
    "            ps: List of output probabilities\n",
    "        Returns:\n",
    "            Gradients\n",
    "        \"\"\"\n",
    "        # Initialize gradients\n",
    "        dWax, dWaa, dWya = np.zeros_like(self.Wax), np.zeros_like(self.Waa), np.zeros_like(self.Wya)\n",
    "        dba, dby = np.zeros_like(self.ba), np.zeros_like(self.by)\n",
    "        dh_next = np.zeros_like(hs[0])\n",
    "\n",
    "        # Loop through each time step\n",
    "        for t in reversed(range(len(targets))):\n",
    "            # Compute dy, the derivative of the loss with respect to the output\n",
    "            dy = np.copy(ps[t])\n",
    "            dy[targets[t]] -= 1\n",
    "\n",
    "            # Compute the gradient of the output layer\n",
    "            dWya += np.dot(dy, hs[t].T)\n",
    "            dby += dy\n",
    "\n",
    "            # Backpropagate the gradient to the hidden layer\n",
    "            dh = np.dot(self.Wya.T, dy) + dh_next\n",
    "            dh_raw = (1 - hs[t] ** 2) * dh # Backprop through tanh\n",
    "\n",
    "            # Compute the gradients of the hidden layer\n",
    "            dWax += np.dot(dh_raw, inputs[t].T)\n",
    "            dWaa += np.dot(dh_raw, hs[t].T)\n",
    "            dba += dh_raw\n",
    "\n",
    "            # Update the next hidden state\n",
    "            dh_next = np.dot(self.Waa.T, dh_raw)\n",
    "\n",
    "        # Clip gradients to mitigate exploding gradients\n",
    "        for dparam in [dWax, dWaa, dWya, dba, dby]:\n",
    "            np.clip(dparam, -1, 1, out=dparam)\n",
    "        \n",
    "        return dWax, dWaa, dWya, dba, dby\n",
    "        \n",
    "    \n",
    "    def backward_adagrad(self, gradients, lr=0.1):\n",
    "        \"\"\"\n",
    "        Update the model parameters using the AdaGrad optimization algorithm.\n",
    "        Args:\n",
    "            gradients: List of gradients\n",
    "            lr: Learning rate\n",
    "        \"\"\"\n",
    "        for param, dparam in zip(['Wax', 'Waa', 'Wya', 'ba', 'by'], gradients):\n",
    "            self.m[param] += dparam ** 2\n",
    "            self.__dict__[param] -= lr * dparam / np.sqrt(self.m[param] + 1e-8)\n",
    "\n",
    "\n",
    "    def sample(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5. The `sample` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    # Simple \"vanilla\" RNN model\n",
    "\n",
    "    def __init__(self, vocab_size, hidden_size=64):\n",
    "        # Store the model hyperparameters\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Initialize the model weights and biases\n",
    "        self.Waa = np.random.randn(hidden_size, hidden_size) / 1000\n",
    "        self.Wax = np.random.randn(hidden_size, vocab_size) / 1000\n",
    "        self.Wya = np.random.randn(vocab_size, hidden_size) / 1000\n",
    "        self.ba = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((vocab_size, 1))\n",
    "        \n",
    "        # AdaGrad optimization parameters\n",
    "        self.m = {k: np.zeros_like(v) for k, v in self.__dict__.items() if isinstance(v, np.ndarray)}\n",
    "        \n",
    "\n",
    "    def forward(self, inputs, h_prev):\n",
    "        \"\"\"\n",
    "        Perform forward pass.\n",
    "        Args:\n",
    "            inputs: One-hot encoded inputs of shape (vocab_size, seq_length)\n",
    "            h_prev: Initial hidden state of shape (hidden_size, 1)\n",
    "        Returns:\n",
    "            outputs: List of output probabilities\n",
    "            hs: List of hidden states\n",
    "        \"\"\"\n",
    "        # Initialize hidden state, logits, and output pobabilities\n",
    "        hs, ys, ps = {}, {}, {}\n",
    "        hs[-1] = np.copy(h_prev)\n",
    "\n",
    "        for t, x in enumerate(inputs):\n",
    "            # Update the hidden state\n",
    "            hs[t] = np.tanh(np.dot(self.Wax, x) + np.dot(self.Waa, hs[t-1]) + self.ba)\n",
    "\n",
    "            # Compute the output logits\n",
    "            ys[t] = np.dot(self.Wya, hs[t]) + self.by\n",
    "\n",
    "            # Softmax probabilities\n",
    "            ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
    "\n",
    "        return ps, hs\n",
    "    \n",
    "\n",
    "    def compute_gradients(self, inputs, targets, hs, ps):\n",
    "        \"\"\"\n",
    "        Compute the gradients for one sequence.\n",
    "        Args:\n",
    "            inputs: List of one-hot encoded inputs\n",
    "            targets: List of integer targets\n",
    "            hs: List of hidden states\n",
    "            ps: List of output probabilities\n",
    "        Returns:\n",
    "            Gradients\n",
    "        \"\"\"\n",
    "        # Initialize gradients\n",
    "        dWax, dWaa, dWya = np.zeros_like(self.Wax), np.zeros_like(self.Waa), np.zeros_like(self.Wya)\n",
    "        dba, dby = np.zeros_like(self.ba), np.zeros_like(self.by)\n",
    "        dh_next = np.zeros_like(hs[0])\n",
    "\n",
    "        # Loop through each time step\n",
    "        for t in reversed(range(len(targets))):\n",
    "            # Compute dy, the derivative of the loss with respect to the output\n",
    "            dy = np.copy(ps[t])\n",
    "            dy[targets[t]] -= 1\n",
    "\n",
    "            # Compute the gradient of the output layer\n",
    "            dWya += np.dot(dy, hs[t].T)\n",
    "            dby += dy\n",
    "\n",
    "            # Backpropagate the gradient to the hidden layer\n",
    "            dh = np.dot(self.Wya.T, dy) + dh_next\n",
    "            dh_raw = (1 - hs[t] ** 2) * dh # Backprop through tanh\n",
    "\n",
    "            # Compute the gradients of the hidden layer\n",
    "            dWax += np.dot(dh_raw, inputs[t].T)\n",
    "            dWaa += np.dot(dh_raw, hs[t-1].T)\n",
    "            dba += dh_raw\n",
    "\n",
    "            # Update the next hidden state\n",
    "            dh_next = np.dot(self.Waa.T, dh_raw)\n",
    "\n",
    "        # Clip gradients to mitigate exploding gradients\n",
    "        for dparam in [dWax, dWaa, dWya, dba, dby]:\n",
    "            np.clip(dparam, -1, 1, out=dparam)\n",
    "        \n",
    "        return dWax, dWaa, dWya, dba, dby\n",
    "\n",
    "\n",
    "    def backward_adagrad(self, gradients, lr=0.1):\n",
    "        for param, dparam in zip(['Wax', 'Waa', 'Wya', 'ba', 'by'], gradients):\n",
    "            self.m[param] += dparam ** 2\n",
    "            self.__dict__[param] -= lr * dparam / np.sqrt(self.m[param] + 1e-8)\n",
    "\n",
    "\n",
    "    def sample(self, seed_char, char_to_ix, ix_to_char, n=50):\n",
    "        \"\"\"\n",
    "        Generate text by sampling from the model.\n",
    "        Args:\n",
    "            seed_char: Seed character\n",
    "            char_to_ix: Dictionary mapping characters to integers\n",
    "            ix_to_char: Dictionary mapping integers to characters\n",
    "            n: Number of characters to sample\n",
    "\n",
    "        Returns:\n",
    "            Sampled text\n",
    "        \"\"\"\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        x[char_to_ix[seed_char]] = 1\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "        output = []\n",
    "\n",
    "        for t in range(n):\n",
    "            h = np.tanh(np.dot(self.Wax, x) + np.dot(self.Waa, h) + self.ba)\n",
    "            y = np.dot(self.Wya, h) + self.by\n",
    "            p = np.exp(y) / np.sum(np.exp(y))\n",
    "            idx = np.random.choice(range(self.vocab_size), p=p.ravel())\n",
    "            output.append(ix_to_char[idx])\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "\n",
    "        return ''.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/rnn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Training the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1. Helper functions\n",
    "Complete the following helper functions below by filling in the `TODO` flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode a character\n",
    "def one_hot_encode(idx, vocab_size):\n",
    "    \"\"\"One hot encode a character of the vocabulary given its index.\n",
    "    Args:\n",
    "        idx: Index of the character in the vocabulary\n",
    "        vocab_size: Size of the vocabulary\n",
    "\n",
    "    Returns:\n",
    "        One-hot encoding of the character\n",
    "    \"\"\"\n",
    "    one_hot = np.zeros((vocab_size, 1)) #TODO: Complete\n",
    "    one_hot[idx] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/onehot.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a sequence of characters to one-hot encoded vectors\n",
    "def encode_sequence(sequence, char_to_ix, vocab_size):\n",
    "    \"\"\"Convert a sequence of characters to one-hot encoded vectors.\n",
    "    Args:\n",
    "        sequence: Input sequence\n",
    "        char_to_ix: Dictionary mapping characters to integers\n",
    "        vocab_size: Size of the vocabulary\n",
    "\n",
    "    Returns:\n",
    "        One-hot encoded sequence\n",
    "    \"\"\"\n",
    "    return [one_hot_encode(char_to_ix[ch], vocab_size) for ch in sequence] #TODO: Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/encode.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text during training\n",
    "def sample_text(rnn, seed_char, char_to_ix, ix_to_char, length=200):\n",
    "    \"\"\"Sample text from the model during training.\n",
    "    Args:\n",
    "        rnn: RNN model\n",
    "        seed_char: Seed character\n",
    "        char_to_ix: Dictionary mapping characters to integers\n",
    "        ix_to_char: Dictionary mapping integers to characters\n",
    "        length: Number of characters to sample\n",
    "\n",
    "    Returns:\n",
    "        Sampled text\n",
    "    \"\"\"\n",
    "    return rnn.sample(seed_char, char_to_ix, ix_to_char, length) #TODO: Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/sample.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2. Sanity checks\n",
    "To ensure that we have not made an implementation error, we can pass a sentence from the training set through the network. Since the network has not yet been trained, we should find that the caharacters in this sentence are all roughly equally likely, i.e., an array of probabilty vectors with all elements approximately equal to `1 / _vocab_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1767751768632382e-07\n",
      "1.8033913600640217e-07\n",
      "1.69712655782861e-07\n",
      "2.8199625395734784e-07\n",
      "2.0155049529173297e-07\n",
      "1.5795392535947195e-07\n",
      "2.165698145522449e-07\n",
      "1.492697899539014e-07\n",
      "2.8224446797786573e-07\n",
      "1.566537881306923e-07\n",
      "2.0219454098524547e-07\n",
      "2.17004415698982e-07\n",
      "2.639646829397796e-07\n",
      "1.79919697184186e-07\n"
     ]
    }
   ],
   "source": [
    "# Initialize the RNN\n",
    "hidden_size = 64\n",
    "rnn = RNN(vocab_size, hidden_size)\n",
    "\n",
    "inputs = 'Under the rain'\n",
    "inputs = encode_sequence(inputs, char_to_ix, vocab_size)\n",
    "h_prev = np.zeros((hidden_size, 1))\n",
    "out, _ = rnn.forward(inputs, h_prev)\n",
    "for ps in out.values():\n",
    "    print(np.max(ps - (1 / vocab_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check that our RNN samples random characters, which look like the characters sampled uniformly from the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled text with RNN:\n",
      "l5WPïÏgiDkàGl»è(#Èi”QO5D!ôMh/'à—%Blt7zco,JUKç”%:862Sh eX:tàF2âxrmt7*KW™à/âomêâQ\n",
      "l.]tv[ûùâJDsmyAHrëOû?É5u6)Nê7Pq7vo:2])vTcJ;2)‘R_jWà$TnB:[Vgt﻿3kB“lCSG0«,“ëVX_crL’V5IîJlaPgÉRtNS“hC.$!XkPÈx2WP:]q)éC.UâKô \n",
      "\n",
      "Sampled text with uniform distribution:\n",
      "x[-1ÈN\n",
      "KG)èm—fT;j('\n",
      "™Hg!-'1WLGéBqO—9ùïpü[ k\n",
      "E9•-K\n",
      "o:ÈqB’JQà'ïa'-K_TF7ÏàU﻿Y\n",
      "»2hI9êciÈàpëV[ô«O—﻿’7WÏJ«VJE6]«ùJ;ybGNee[9T_«(JTxXGr•üàgsv’à$™0î!é«4LcrÈ3,GPg9P«zàpSAkDx‘Ïee9s’û(ïû1rkàzÈÏQ”K\n",
      "/èÏhK‘ôsYàzbwB“\n"
     ]
    }
   ],
   "source": [
    "# Check that the RNN samples random characters\n",
    "seed_char = 'a'\n",
    "n = 200\n",
    "sampled_text = sample_text(rnn, seed_char, char_to_ix, ix_to_char, n)\n",
    "print(\"Sampled text with RNN:\")\n",
    "print(sampled_text, '\\n')\n",
    "sample_text_unif = np.random.choice(chars, n)\n",
    "print(\"Sampled text with uniform distribution:\")\n",
    "print(''.join(sample_text_unif))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3. Training function\n",
    "Complete the `TODO` flags below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_rnn(rnn, data, char_to_ix, ix_to_char, seq_length=25, num_iterations=100_000, print_every=1000):\n",
    "    loss_history = []\n",
    "    h_prev = np.zeros((rnn.hidden_size, 1))\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        # Create a mini-batch\n",
    "        start_idx = iteration % (len(data) - seq_length - 1)\n",
    "        inputs = data[start_idx:start_idx + seq_length]\n",
    "        targets = data[start_idx + 1:start_idx + seq_length + 1]\n",
    "        \n",
    "        # One-hot encode inputs and targets\n",
    "        inputs_encoded = encode_sequence(inputs, char_to_ix, rnn.vocab_size)\n",
    "        targets_encoded = [char_to_ix[ch] for ch in targets]\n",
    "        \n",
    "        # Forward pass\n",
    "        ps, hs = rnn.forward(inputs_encoded, h_prev)\n",
    "        \n",
    "        # Compute gradients\n",
    "        gradients = rnn.compute_gradients(inputs_encoded, targets_encoded, hs, ps)\n",
    "        \n",
    "        # Backward pass (parameter update)\n",
    "        rnn.backward_adagrad(gradients)\n",
    "        \n",
    "        # Loss computation\n",
    "        loss = -np.sum([np.log(ps[t][targets_encoded[t], 0]) for t in range(len(targets_encoded))]) #TODO: Compute the loss using the formula for the negative log likelihood\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        # Print loss and generate sample text periodically\n",
    "        if iteration % print_every == 0:\n",
    "            print(f\"Iteration {iteration}, Loss: {loss}\")\n",
    "            seed_char = data[start_idx]\n",
    "            sample = sample_text(rnn, seed_char, char_to_ix, ix_to_char, 200)\n",
    "            print(f\"Sample Text:\\n{sample}\\n\")\n",
    "\n",
    "    # Plot the loss history\n",
    "    plt.plot(loss_history)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4. Train the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 116.8207029888842\n",
      "Sample Text:\n",
      "W\n",
      "Cnn4u-g'5yvçjqf?OR•8yù%»P?##ùFDMi*îsÏ%9jJu358gèG!pôW*É!*daÈ?2rx.oWmjg?/jO]3$ô(5™5RvCNüy5Tç,q:ÉdyKê﻿pmGÏL T ÉJt”GJWf'È2P àSw41oc™Egù4Xâ([9-ÉRÏ,*ùBchû!gg﻿qUq76inÏÈd™D[•x%/dGhDWlAT8S”i'\n",
      "$PQn?bGz ùÏHékï\n",
      "\n",
      "Iteration 1000, Loss: 55.297705286908496\n",
      "Sample Text:\n",
      "\n",
      "\n",
      "UBL—udi'f T:\n",
      "\n",
      " S2 U\n",
      "\n",
      " J P\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Deokretce OAT\n",
      "\n",
      "Pô\n",
      "\n",
      "\n",
      "\n",
      "L\n",
      "E\n",
      "B\n",
      "\n",
      "\n",
      "M\n",
      "M\n",
      "\n",
      "\n",
      "y\n",
      "OUS\n",
      "UPOumsireoMnb JMd S\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "tDing\n",
      "hhUERusinlesliu difmgRethenlse!ulyarOreduuefuis 9e\n",
      "\n",
      "*fM*\n",
      "\n",
      "\n",
      "\n",
      "U S\n",
      "\n",
      "\n",
      " 2n\n",
      "\n",
      "\n",
      "EORwL\n",
      "e\n",
      "FDmOC.T\n",
      "\n",
      "\n",
      "S\n",
      "S\n",
      "\n",
      "\n",
      "Iteration 2000, Loss: 45.944865562841855\n",
      "Sample Text:\n",
      " deboml ct _obeneugietis bébinte sovértbthaueteutmm piun B\n",
      "aumsr .e B4 bobeym ps youeunt _Ffaurien, qtanl, ne cé nettis 2u mé phopete té n\n",
      " _bonk qs bé_e nnMit laitse atiamet iures botene cé te tart i\n",
      "\n",
      "Iteration 3000, Loss: 50.26824036190459\n",
      "Sample Text:\n",
      "ore noumentbe àUpare\n",
      "co déire à inpoom séuarene de érchériceemqelie taimenlar, an pauttorepletire set pevare p raure couselie\n",
      "derititarr à Mains avaratit larie steraenwaicrlenbeme àaresuerene à re, la\n",
      "\n",
      "Iteration 4000, Loss: 53.8041585841441\n",
      "Sample Text:\n",
      "e,  m•chéitCe\n",
      "IEtCoumamYtbele (IJdec) m6t pItopToqIux Fre, m8utiu _efi-27 maûse () l'eu dondonile s'Eychéurmess pafit MomméCe daelyaLatotet\n",
      "Ee\n",
      "dangéc'\n",
      "Ffament de BêETaére m85 poût\n",
      "tBanEes qle damarEe(\n",
      "\n",
      "Iteration 5000, Loss: 56.92683404950457\n",
      "Sample Text:\n",
      "nbali, émtafantaint toà. nan petLatjanPaénsamdnen ens ne _FronsatoxPéndtandee Le cram#ètéson sraicemées lér, anorta_ de sorsilils re nèéosonlamraracriér phoumdet juntens létirna pon hocconderaivaracan\n",
      "\n",
      "Iteration 6000, Loss: 58.79292935669929\n",
      "Sample Text:\n",
      "»rC l en'int, que Bal aur d.mme\n",
      "Bantet\n",
      "autar'\n",
      "d'ouilacue lLtraut Bun an Bi2 lauresge »L ldh, KLue feite H'eulelin » Framtauulaprou det cain Jdon Buule: s van au. ac'adaies s leuen c avtacêeule nacfiba\n",
      "\n",
      "Iteration 7000, Loss: 42.48846666876497\n",
      "Sample Text:\n",
      "emre, te be5me l'Eige sepbele qu'v'altes, mrebpant mais\n",
      "éotse\n",
      "soun, »raucovejums, éx Ïa des sonm, égapre cogtè. sr. avéail, se re\n",
      "poon9 Pen abres se famvaule paitos et somvess éêt qu', que\n",
      "Jant-SÉ\n",
      "lam\n",
      "\n",
      "Iteration 8000, Loss: 51.96505768734405\n",
      "Sample Text:\n",
      "lien ent\n",
      "ce. D'envéraiet et pitlarnes de seqhlu en amosfes lé meavrautee leuye\n",
      "coiqeu se touvemprsauues qua manvie e en it c'oit\n",
      "ales et nistarifie le n'ait pes\n",
      "codene vauxes\n",
      "s'aia. arryêlos. Iune dis\n",
      "\n",
      "Iteration 9000, Loss: 55.13857414609623\n",
      "Sample Text:\n",
      " andtétent eOs llt d'utire de pimjraute de\n",
      "Atr giété en méjondie. Lércom utateilen déesltPiéter) le que de que Pulentairt-P. d'hente, qu Aux toitautte, da toy enc ne s'ompe les d'îenfren let Praujlens\n",
      "\n",
      "Iteration 10000, Loss: 63.14496069249514\n",
      "Sample Text:\n",
      "isan ques pansuarse un us à, lu d'à\n",
      "d gaulesèn auieale del-Aù pla, n'alesnas la. n'ilut hassue tlant na dauanoaucicaune jalaiivet pdilà se ront,\n",
      "Sun Bant vaure n'aAn\n",
      "sauces r'aule laude, pralane ne lu\n",
      "\n",
      "Iteration 11000, Loss: 66.05009526708547\n",
      "Sample Text:\n",
      "qu'amrdimit ses dét At_ vaic\n",
      "lur maroitiaient me c'énun ll pbri tt'ichinnée qait aint de à\n",
      "c'antulaavint Seiqumur, r'heis macvorn, ie pébpanmgliait prdi ndi entares Bérdet ment\n",
      "l'aintues sénTi lueuicl\n",
      "\n",
      "Iteration 12000, Loss: 59.186544689216525\n",
      "Sample Text:\n",
      "st ques  lEsmis! tet fenOnt la lorsét  lal. r lusun bovrent béest!\n",
      " \n",
      " -! mhiassont pitaus!\n",
      " rois ur pout chimit! rocbet\n",
      "  Nui!  ruel noysyaalue bélontos n félétaé pos eu nait se pAstiquolet ar vechis!\n",
      "\n",
      "Iteration 13000, Loss: 46.975522440531954\n",
      "Sample Text:\n",
      "ourpis, arhes gan n un nons! L'églin, lu ts\n",
      "  pavis,\n",
      " Lous, nù licdes loman,,\n",
      "  #è   us vesaprine, re fus, lars.\n",
      "\n",
      "  he\n",
      "  Dén un rolle,\n",
      " \n",
      "I beus miie c'eurs s lu la,\n",
      "  AOrge sont shenx plans pair'mauth\n",
      "\n",
      "Iteration 14000, Loss: 46.02791554253537\n",
      "Sample Text:\n",
      "oglii  loirtarsérr meude durnsre d'icardiedess\n",
      "   Pè\n",
      " CHui els are fui Aur gapelex le le lur oubid Eu chain leugje hes l'iiq«  un  am mende nir parbari:\n",
      "  Tèe en-Ce  urs,\n",
      "  It le fui S6oux f'ià  ouN h\n",
      "\n",
      "Iteration 15000, Loss: 61.75427074573444\n",
      "Sample Text:\n",
      "is la lai;\n",
      "  Ou de las jechoire qu'édonsist,\n",
      "  Pr;recble.\n",
      "\n",
      " peltins.\n",
      "\n",
      "  Et rapite des ulêtéta; se centi cortress\n",
      " bais d'oun er veypcméde.\n",
      "  Authon ces; louves re l'itsant rlin, lavec;e corguci;  lili\n",
      "\n",
      "Iteration 16000, Loss: 58.90823441594335\n",
      "Sample Text:\n",
      "es ailen iù  Teptis de seppr'int sottres frecnsix Juel  lus cens le lues, ex\n",
      "  Es onchatedi rers ies\n",
      "  ut sange simirs ler urs deus d'ivéieîs lei aut le neu » luges, on pouther f Da lide,  pen a fuin \n",
      "\n",
      "Iteration 17000, Loss: 75.06295758944766\n",
      "Sample Text:\n",
      "ons!\n",
      "\n",
      "  «\n",
      "\n",
      "\n",
      "  mers,;\n",
      "  Des soumis des!\n",
      "  La dile ler foèmrit\n",
      "\n",
      "\n",
      " Dofrirer7! lauosures\n",
      " te  PSUre ère\n",
      "  At souit\n",
      " mes où pot*sies nsou'eAdis\n",
      "  DAavl filde, »\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  -_\n",
      "\n",
      "\n",
      "  Pourtes m'es maikôdis\n",
      "  Quers\n",
      "\n",
      "Iteration 18000, Loss: 49.86853768893217\n",
      "Sample Text:\n",
      "bait bit les larcher,\n",
      "  -Jue guaves,\n",
      " cectésis,\n",
      "  Lage elvorire.\n",
      "\n",
      "  . TEt des mecfres Vôs mes dins!\n",
      " s les les hez, que,\n",
      "\n",
      "   Em as ses se mage.\n",
      "   Ene.\n",
      "  Puquc daiss,\n",
      " durses lesjoile cepre de Prebfer\n",
      "\n",
      "Iteration 19000, Loss: 54.89741148390426\n",
      "Sample Text:\n",
      "he nagranchet l'or étanm0!\n",
      "  Cane,\n",
      "  Leu monre tilva se lis ju videcte de char leuche que  Que codb sa roracmone duy seg,  al le chedsacte der molant,\n",
      "\n",
      " rale\n",
      "  sLeir da chaTSal est pl darche dempre ag\n",
      "\n",
      "Iteration 20000, Loss: 50.56141819190961\n",
      "Sample Text:\n",
      "  Et fer,re,  Dh rébais et Et coppeus di cô asetes;\n",
      "\n",
      "  Leile\n",
      "  Dain et Maut eù VEs laitiasK\n",
      "\n",
      "  Delisant dée fr polal,\n",
      "\n",
      "\n",
      "\n",
      "  Dires, u lui étan foun e,\n",
      "  L'banaies dul à lé pes\n",
      " Phalgesuvent bies lait,\n",
      " \n",
      "\n",
      "Iteration 21000, Loss: 48.12717631429821\n",
      "Sample Text:\n",
      "en s'oirçe duorc.\n",
      "\n",
      "  Dolier.\n",
      "\n",
      "  Des deaviengies choris viontornes\n",
      "\n",
      "  Ja énaBirairent seuvea san reantent, foyovurdle nelponder* moicsoreil teurin)es tosusunét,\n",
      "\n",
      "  pos des saupiiaves;\n",
      "\n",
      "  Quil,\n",
      "  MDîeya\n",
      "\n",
      "Iteration 22000, Loss: 54.512842490950526\n",
      "Sample Text:\n",
      "ti.\n",
      "  T'lurentelque  vas leu l''ous esfovel lurA lSVoures enreus étaur d'els te Et je toge.\n",
      "  Per de de Bant laur les fure à res qu'is bouve l'iuoreu que g''oure voun des robûter maux6,\n",
      "\n",
      "\n",
      "  Vui se d'e\n",
      "\n",
      "Iteration 23000, Loss: 57.00089005248392\n",
      "Sample Text:\n",
      "cûmégrur no ple fouit hét qui Vzu foyrerqu'onbojêmix,\n",
      "  Qu'iait: da hamrone dant chouHostJur varonni dabomigu fyan'égixsé Gau deu det;\n",
      " JLque\n",
      "  LAdgé d'hant dempfo» pOuux deA dobGiiét loir mail laigcé\n",
      "\n",
      "Iteration 24000, Loss: 46.04895314079361\n",
      "Sample Text:\n",
      "de\n",
      "  Que cee lherson rseu des irsrer do entilit des pous es birdes de don pe meur témxas aut de vesrde cigle ut que tes fLes pas di pLuemin,  E ges elre nes récontqui qulidirs s'f bra demènes la pour \n",
      "\n",
      "Iteration 25000, Loss: 48.85196306394434\n",
      "Sample Text:\n",
      "u do sait sospeler nuvarart\n",
      "\n",
      "  Pates tuante tons eintravales loulique molimens à\n",
      "  Egik\n",
      "  Sait le.\n",
      "  hortues l'ontert aut ron; Es léar.\n",
      "\n",
      "  EN ves, fmann fonnes ernant relis, sis naldse rauviss farar ô\n",
      "\n",
      "Iteration 26000, Loss: 60.23209612379767\n",
      "Sample Text:\n",
      "ou chuuur foirs su mobun Deur heu  Lax irorique je s-omleus pian ase llas sofces, UAuviant pous iù lus quir bes vel hos j'uus ne houliis ragies, Tofte, à Cau d'insur oyun quais len houCréEx,\n",
      "  Et gouf\n",
      "\n",
      "Iteration 27000, Loss: 50.122446627421446\n",
      "Sample Text:\n",
      "ilce deir rec apersurs restr couver trus à donx reéralur j'out dor honEis dumire fesuir seu t'infsacime.\n",
      "  BoIhtont sir liruire bicconciur!  Jes pofnin! ronrair n'Ezfatèn où cumêg ur ôt ans cleur foir\n",
      "\n",
      "Iteration 28000, Loss: 68.51922414930412\n",
      "Sample Text:\n",
      "eclant d'yEt danus âgiquilez seurs moy dan cers;\n",
      "  fuie gubans miabiblé, eartle,\n",
      "  Oe d'irgite,\n",
      "  Cestièluds moyen te jeux son doE.\n",
      "\n",
      " CRèêque émons auecee\n",
      "  Où vapou uteliè soù à\n",
      "  Ca.\n",
      "\n",
      "  Ceumie,\n",
      " dgâ\n",
      "\n",
      "Iteration 29000, Loss: 53.47108785332534\n",
      "Sample Text:\n",
      "ant ge toi jou sous l'Au teô rimeme!\n",
      "\n",
      "  Pomars tort.\n",
      "\n",
      "  Et dor metnB deux morgt ma taîcelent à de de  le palesé.\n",
      "\n",
      "  Arfeux encald!\n",
      "  E Varril de d'iurolxe do TTa bpretse Thau surs d'ER lont! ôFr ôt ch\n",
      "\n",
      "Iteration 30000, Loss: 57.507457892930894\n",
      "Sample Text:\n",
      "tains\n",
      "\n",
      "\n",
      "  Mescle ce qvê fuche fimmiont\n",
      "  Dls at'terx,ux Tun Ma ste coux du qiont\n",
      "  Ot jA OT Ta mone!\n",
      "\n",
      "\n",
      "\n",
      "  E- saux! Mominc panx Hoclun ll Oz Ja cour.  Dus, â.\n",
      "\n",
      "\n",
      "  Tui Es choupquifl\n",
      "  Dul sY un que pu b\n",
      "\n",
      "Iteration 31000, Loss: 67.73656265348677\n",
      "Sample Text:\n",
      "ors, ôt l'ant mafLes boforiselrex,\n",
      "  Pazet nou Besjale fu ce Bédre, femmbens;\n",
      "  Qun deant l'ale d'poux mur hoon pEt tépruep d'auis un pechamas in u'urqudeu st romaBpalobece,\n",
      "  QueT s merdale biabrourb\n",
      "\n",
      "Iteration 32000, Loss: 61.47389005595396\n",
      "Sample Text:\n",
      "âes erlentoquanout vèen urdens, coès es sa, etrs tes-le!,\n",
      "  Ou Vue, let lour etlcenence l'urs ain,, l'ilique d'euin! preulses mèna on!\n",
      " .\n",
      "\n",
      "  THeydulre, hiètes où le\n",
      " Jes coutis eivrens commegquet som0\n",
      "\n",
      "Iteration 33000, Loss: 47.270763287166844\n",
      "Sample Text:\n",
      "rs ôs dès, gouraque d'oute ut pux,\n",
      "  Lè lin mèse ma deur hardirr coOfre\n",
      "  Dus loumeux de memmra envour! mot el Sonture?\n",
      "\n",
      "  hanur se pransime j'emile à ars l'urou le cour ne courd!\n",
      " Je des arsie et don\n",
      "\n",
      "Iteration 34000, Loss: 59.21874590459647\n",
      "Sample Text:\n",
      "choubure,\n",
      "  Poaus me.\n",
      "  pun amét jaater du jansre!\n",
      "  Et di nlun l'Sainfu oupquenx de geirs êvre vu tse!\n",
      "  Evurens tanfude lu ugren ât. L'utjarzis an des cobunul laa, de danus,\n",
      "\n",
      "  un Lan vun bonveitrav\n",
      "\n",
      "Iteration 35000, Loss: 55.17825416206518\n",
      "Sample Text:\n",
      "de ilennen les sucs, vâmen,\n",
      "  Perléperse,\n",
      "  Seprasrers tra otis-tet et common, éte\n",
      "  Cor pacare,\n",
      "  Lu pour la ces de le*tu ponubine,\n",
      "  Pé2hrsenne colure sê pla de pémyoyee le féiruera, jan civix îengé\n",
      "\n",
      "Iteration 36000, Loss: 47.76539387471935\n",
      "Sample Text:\n",
      "  Toux âimbene,\n",
      "\n",
      "  Palprales\n",
      "  A Ne f tra dels out le,\n",
      "  AFr avetx tge diant lâ on commaplanchome mat mox mam t'oc epovumen.\n",
      "\n",
      "  LI A Iar sanu s bé ronur séplus ppeide avix majacre de la Bemmanse\n",
      "\n",
      "  HA\n",
      "\n",
      "Iteration 37000, Loss: 41.09774248105261\n",
      "Sample Text:\n",
      "berain et la ciénèténe.\n",
      "\n",
      "  Se mome sholaree mentu,\n",
      "  te paavèire\n",
      "        Sens âme\n",
      "  UM Conde dout de dole dont  Od rin v'onilee,\n",
      "  Qu bePHâchond d'itu vour avore l'on de bout hels aisse t'asgof ces ou\n",
      "\n",
      "Iteration 38000, Loss: 41.95742507465521\n",
      "Sample Text:\n",
      "é fance téferce qu'unems uns sib colezuvéte\n",
      "  Ce ut,\n",
      "  J vêre un blors:\n",
      "  Le sue,\n",
      "  O cumn,\n",
      "    pardaHtibaun âque quue en sune Bu consait abie\n",
      "   vu toncon, eu tole se pourd;\n",
      "  A  Complie eres taDafmo\n",
      "\n",
      "Iteration 39000, Loss: 37.30068421105843\n",
      "Sample Text:\n",
      "u'âmenx,\n",
      "  Qu vonéBoimdio!\n",
      "\n",
      "\n",
      "\n",
      "  Hèreane.\n",
      "  L'omarces\n",
      "  Le mausecle\n",
      "  Oue pyoi à voiqumldantab àt mis\n",
      "  Et jnurs itraie cumidés\n",
      "  Du sDèé j'e çus me chêjesis dez L'habil mesr9Hsamon. 4i sorve\n",
      "  D'Egits\n",
      "\n",
      "Iteration 40000, Loss: 52.188469537045165\n",
      "Sample Text:\n",
      "memme leuvine lnoie immmîb trâmez, xoifque autç.\n",
      "\n",
      "  Don jotche à l'avoste l'T-Compi,\n",
      "\n",
      "  L'é mGmainda deplaliple\n",
      "  Eogs pospils, errouC, la vonse\n",
      "  Vens bomoù me aile le  etâmes\n",
      "  Cmis lemmmé\n",
      "\n",
      "\n",
      "   serû\n",
      "\n",
      "Iteration 41000, Loss: 51.3835462788138\n",
      "Sample Text:\n",
      "rongéels sense cempueus meideste fédens petrus.\n",
      "\n",
      "\n",
      "  Uâce,\n",
      "  Tun ent âpre, pé-dons,\n",
      "  Theus cé les mes laU mas ila coimnele uatusé soutrout  non lépeefess lEn paises deunir qua geleinir frire quiseit t\n",
      "\n",
      "Iteration 42000, Loss: 63.77345176864106\n",
      "Sample Text:\n",
      "e yulaple\n",
      "  paute da soit motlasur la gleamése, doux,\n",
      "  Lan jorntits on measté\n",
      "  -t harneeses eù  Tsas peurendis-sort te te ronges!\n",
      "  Le voimfes, vis que movoir qeu semvesne lemmorrems, mes moiretr mo\n",
      "\n",
      "Iteration 43000, Loss: 49.65550214728547\n",
      "Sample Text:\n",
      " bondes\n",
      "  Qu'oulles.\n",
      "\n",
      "  Quests s'esdurairo? LE roless!\n",
      " prures plaime, enmineves soivsone!\n",
      "\n",
      "  Quivareas le illor! bisre sondis!\n",
      "  Ieit ols anoique ent soiront\n",
      "  Et relsessoma! somque de berablawisd'ou\n",
      "\n",
      "Iteration 44000, Loss: 64.22789983542023\n",
      "Sample Text:\n",
      "oe ripres sontr'un paume,\n",
      "  Le veeie! dument son de pumais s'aquaine Ma meuisuyis!\n",
      "\n",
      " E$ Onnt\n",
      "  Tei rent ne  bas que le perpuis end ta fond l'imin,\n",
      "\n",
      "  Momfes ches brpuses plareu:  Corest de fonens ces \n",
      "\n",
      "Iteration 45000, Loss: 54.37985670645787\n",
      "Sample Text:\n",
      "irqus n'us endés ins à\n",
      "s junil'bfmpigsu, noMgauN fiquen,! \n",
      "\n",
      "   uon ne Proille\n",
      "\n",
      "  Os , cnapte dus pliarbié caines\n",
      "s l'ergle l''arjestrers ces aux engan uongerl, apour!\n",
      "  J'étésan nimèbee Beau, qu'étaro\n",
      "\n",
      "Iteration 46000, Loss: 60.37840329348193\n",
      "Sample Text:\n",
      " nais lu Boit piiders-de d'de coqui tesaisse st oe-p'aires.\n",
      "\n",
      " Juvies de eu songe, de d'ueres.\n",
      "\n",
      "  Ce cimansi,\n",
      "  Tesprecrse,\n",
      "\n",
      "  Apreremig darbéne drendtase et prange: de uve sueur\n",
      "\n",
      "  IIM\n",
      "\n",
      "\n",
      "   Enfeleis u\n",
      "\n",
      "Iteration 47000, Loss: 49.489648803476186\n",
      "Sample Text:\n",
      "cie, se vest?\n",
      "\n",
      "  Tr_\n",
      "  J as jais las clufre ux de cruix,\n",
      "  Cos fir! mat vêjauvre se vohare touroives titat, de redeir voncisel un ons?\n",
      "\n",
      "  Ju det\n",
      "  Deux dur ule,\n",
      "  Tone soouse da vinree de veis des mma\n",
      "\n",
      "Iteration 48000, Loss: 58.48858778366526\n",
      "Sample Text:\n",
      "est noc aziuu duve lursammon, gâmle sorquuier,\n",
      "  Cont ton un en sont ent nu four, soqui en ce!\n",
      "\n",
      "  Jue ons drd, crs trands,\n",
      "  Véonne, hommme en pinais moeumes contque nu nogn!  a ti soyé luprret voimet\n",
      "\n",
      "Iteration 49000, Loss: 71.12347110551498\n",
      "Sample Text:\n",
      "Conse la me boloutie à mon ronoi sotie! hoien, reux où à cou toforles racils ul cosfe lent\n",
      "  EMTun jet ta cambeuse\n",
      "  Fofmmper,\n",
      "  E\n",
      " Corsis, êtémeilel\n",
      "  Et tos ovait\n",
      "  «Battez entre ponts enget,\n",
      "  Etré\n",
      "\n",
      "Iteration 50000, Loss: 46.2118257530454\n",
      "Sample Text:\n",
      "abmis ésaise de colacsan. »e,  Jai dane,\n",
      "\n",
      "  Spince, maite loumre die\n",
      "\n",
      "  Que ins la moes ets rateplileubies cOne cassrend'aut P'oAT lAbreile es manpeore n's'empore voun l'avoielate blavépubaise,\n",
      "\n",
      "  Et \n",
      "\n",
      "Iteration 51000, Loss: 51.17766236447137\n",
      "Sample Text:\n",
      "é mou frà\n",
      "  Jo chacsamits, les teil, ut mast l'aile céproinfque conmene la paite que lètre d'a ceène ent et la copvaui lin bafceXlorrange, lhe cha torâcest, cyira courbe prongar chouilveange\n",
      "  Dent do\n",
      "\n",
      "Iteration 52000, Loss: 48.154717470399454\n",
      "Sample Text:\n",
      " lant\n",
      "    Jan loit meleçne corve à ue bagred vandues\n",
      "  Et ve reun el  s nta fon-d;\n",
      "\n",
      " \n",
      "\n",
      "  Où ce tibrame!  Ua Pitat de fadi berude lan,\n",
      "  Et  coïce n'ijernis,\n",
      "  Acofse\n",
      "  Vu que sur et cofant! Paes que, \n",
      "\n",
      "Iteration 53000, Loss: 60.38327936571725\n",
      "Sample Text:\n",
      "Qu déron Don reiranfabaraut men lous re forvegre de commeren el du moun et re contl llone câe ja porges,\n",
      "    es le forse, rard de de don titiri rant en toe crcumyaon poi er un es le vous que rôtir, en\n",
      "\n",
      "Iteration 54000, Loss: 55.937312946451904\n",
      "Sample Text:\n",
      "LUmous moduvend la ceène et faine,\n",
      "\n",
      "  LO ir;\n",
      "   Dobri,\n",
      "  Et mont6s lense,\n",
      "  Te la mont,\n",
      "  Tin  partant\n",
      "\n",
      "  L4 jorfure bes m'hapr,\n",
      "   Cerrile,\n",
      "  Li vans fadre poure\n",
      "  Dis di cre,\n",
      "\n",
      "  Dle ex on pclorne,\n",
      "\n",
      "\n",
      "\n",
      "Iteration 55000, Loss: 50.40847359828491\n",
      "Sample Text:\n",
      " Lésége;\n",
      "\n",
      "  V-Tammaques de chsre brime,\n",
      "  LEt « ur core d'etrte\n",
      "  Qu'et en , le tui de mons.\n",
      "\n",
      "\n",
      "   Est chée;\n",
      "  ARAit dammieuet pele tonalent sirx\n",
      "\n",
      "  Tois,\n",
      "  Chori pos vis feres ;1 fane défaimme en pest\n",
      "\n",
      "Iteration 56000, Loss: 37.72855685833101\n",
      "Sample Text:\n",
      "ha Focetepne, es es auvaupjedanese tojpe geun te var s'a betaise e'pachaives à im entarees qui citerge.\n",
      "   Et parile eu  Tibmoux,\n",
      "  Queur ton âne et que pouvoipdere;\n",
      " s gantoité, au j'aut bouniar.\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "Iteration 57000, Loss: 44.93728916004589\n",
      "Sample Text:\n",
      "ourent, à prrois on coose,\n",
      "  Toume, un syais r'esé  s brares fandrouûes,\n",
      "  Feur Chracêde ton no dox,\n",
      "  crperé,\n",
      "  Taes ut poxs, on corris, et jéjamat pr'awfonge dt gourge au «   Des qui senma modes\n",
      "   \n",
      "\n",
      "Iteration 58000, Loss: 55.93979504977389\n",
      "Sample Text:\n",
      "nt qus « Chint ex et padujlaine\n",
      "  Til De cele;\n",
      "  L'asue aureubres curp ot rous, les blaimve rantimesé l'ajeeis dane noux!\n",
      "\n",
      "   l ces lemtles et qui ridreds quoel dtloille veûte marvéques, la sardi,\n",
      "  Q\n",
      "\n",
      "Iteration 59000, Loss: 68.83067820149031\n",
      "Sample Text:\n",
      "qu'ilen, es cerin!\n",
      "\n",
      "     LUint éQue famie, lT  fes it cets,\n",
      "     Et et danole? le-trais et t'aIse bas covirrent;\n",
      " ja téotrans jates l'abréces\n",
      "     Pessore, da vertale?\n",
      "    bye?\n",
      "\n",
      "\n",
      "  IU trasé, un lalat \n",
      "\n",
      "Iteration 60000, Loss: 66.36246123556701\n",
      "Sample Text:\n",
      "êTAun abit, flaut Au   Pout plcres!\n",
      "\n",
      "\n",
      "\n",
      "  Lois!\n",
      "\n",
      "  Lyéthaclés.\n",
      "\n",
      "\n",
      "  ET\n",
      "\n",
      "  Ces biù\n",
      "\n",
      "  Que lalétipb aut peun. que silà l'appauné, qui des nourait se pAIûbos,\n",
      "  Ptainef breéntrez lan,\n",
      "  SA bailes che. ux\n",
      "\n",
      "\n",
      "\n",
      "Iteration 61000, Loss: 57.577670031468344\n",
      "Sample Text:\n",
      "émies\n",
      "  Me tllert\n",
      "  L'a véMiait\n",
      "  INE\n",
      "\n",
      "\n",
      "  Tours\n",
      "\n",
      "  Lebmre eu plair moux leil le telnabclincréN.\n",
      "\n",
      "  J'envailer, qun etargee toui! boulaFqu'ain du metI S'horoble lu lyons gomà donge aut à dorde et mévar\n",
      "\n",
      "Iteration 62000, Loss: 47.21362537670861\n",
      "Sample Text:\n",
      "ine nemme\n",
      "  Memmiaurerle-ta? Nu verrerre\n",
      "   rautsis se fétor dul iaate lan tablêshe\n",
      "  Eb d'astr,\n",
      "  De prent charpurs,\n",
      "  TaANafûles narset êtai ta taules\n",
      "\n",
      "  Tou gor;\n",
      "  Que l'à le Dinm pêre.\n",
      "\n",
      "\n",
      "  Souge-t\n",
      "\n",
      "Iteration 63000, Loss: 47.52385903190758\n",
      "Sample Text:\n",
      ", anR paunie,\n",
      "  Manté,\n",
      "  Sa Couuxes vu Sousiez beanves!\n",
      "\n",
      "\n",
      "  Sumeur da moîante\n",
      "  L charme-coux boure?;\n",
      " Sambivaies feles le Tu pouxt.\n",
      "\n",
      "  L'é jesfaun leant da jage?\n",
      "- Sonone augaill liess!\n",
      "\n",
      "  Etoul plen\n",
      "\n",
      "Iteration 64000, Loss: 33.08298284506895\n",
      "Sample Text:\n",
      "t'Ens dombreur seuse, pê saires!\n",
      "\n",
      "  Nhoumed[sain, hoou galêrcle.\n",
      "\n",
      "  Et chendolens.\n",
      "\n",
      "\n",
      "  AVENtE funne, combléy mEs rèlres elqu'2que lus mmêtes aiteu,  le la glerchant leriaier.\n",
      "\n",
      " ENAND* CAson gofrirmeur\n",
      "\n",
      "Iteration 65000, Loss: 48.918351259105336\n",
      "Sample Text:\n",
      "me.\n",
      "\n",
      "  les sennoine cerchentres ceveitranse que toil, sei,\n",
      "  POux.\n",
      "\n",
      "  Maur.\n",
      "\n",
      "  Qur sisrhent ard.\n",
      "    Ra fard. Et fanse le ceure\n",
      "  Tummet.\n",
      "\n",
      "  Amon das mtayétyuns!\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "  le se lis sersues êtes enNEms n\n",
      "\n",
      "Iteration 66000, Loss: 49.0875294903297\n",
      "Sample Text:\n",
      "quere, le dant, selle,rô commetr et lagnes,\n",
      "  Atim de gle,  Iyis câmé ut es chansie!\n",
      "\n",
      "\n",
      "\n",
      "  Toit de aure pretux,\n",
      "  Et sls det dainchâ9uà favre!\n",
      "\n",
      "  La sûsan'oraise\n",
      " »\n",
      "\n",
      "  Lue du s'éprraut soriè olns chesf\n",
      "\n",
      "Iteration 67000, Loss: 57.0035602956269\n",
      "Sample Text:\n",
      "icer se gaié szise tu fun!  ane\n",
      " DAnt leux mme lanmit le mant mond t'ilrine re na gapreux un qui eux feur qu'it es suple et j'aque coni éofmeux me von soit es voû chse couves da mour en Dbe le ni ma D\n",
      "\n",
      "Iteration 68000, Loss: 61.3722826674739\n",
      "Sample Text:\n",
      "chaibierbe de;\n",
      "\n",
      "  Vonnere voén'unda coun mesyaélest bi tsaut\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    roment enpomontes que tes mhuns-mours que petimes enlesse les resés nu pabmes jeslrin  mant se  lamesande\n",
      "  Lers bl lelis.\n",
      "\n",
      "\n",
      "   tup\n",
      "\n",
      "Iteration 69000, Loss: 60.89100701723865\n",
      "Sample Text:\n",
      "tène du de blâctules\n",
      "  LA Pétai.\n",
      "\n",
      "  LE Pamantà concomme da moitentir et ve suvique,\n",
      "  B'lais mastés\n",
      "  Dos, epples sotorne-d'eu s'unhinancet \n",
      "  Et veux bantutanmes à etbemet la soour,\n",
      "  LEbrsi;\n",
      "  Adess\n",
      "\n",
      "Iteration 70000, Loss: 54.86062185937756\n",
      "Sample Text:\n",
      " en\n",
      " lE fuèpet boule sou vyoas\n",
      "  Cannse pautd ux ends édalbent  lint baouvinné lé des dalont de boute\n",
      " s cocroimle de ment paiment di  leug *en bamendeux luse des fant mout-me d'ande;\n",
      " Contes h'uile!,\n",
      "\n",
      "Iteration 71000, Loss: 50.65703118290948\n",
      "Sample Text:\n",
      "'ure, le plend fulre et de pruité un nret que ôle pronde de fechei grous sebre de joravos geistont lule d'antrs, lhenres leurois ses.\n",
      "  Dambes en  lanstéquearez\n",
      "  Pouillee, lon leté,\n",
      " ve moun, l'encit\n",
      "\n",
      "Iteration 72000, Loss: 37.99295444171778\n",
      "Sample Text:\n",
      "erlona su clec flumaydu da siaule de tor cevéré d'os pinta\n",
      "  Ne rour cepruge combièt s'erdé bintét\n",
      "  Ne commimeux boeuroimbs yet banavane cobrain mdfsgent dous qu'aine d'on bamanti,\n",
      "  Sivrant comr die\n",
      "\n",
      "Iteration 73000, Loss: 55.70838333931998\n",
      "Sample Text:\n",
      "nt le tempbacor tas(as duSitbes,\n",
      "  Et que si hébromente,\n",
      "  Il mantchacseux,\n",
      "  Quets qde auls des criest\n",
      "  Et tuis\n",
      " E\n",
      "  Que sovirètinse ddésres-baDmartins telièle\n",
      "  Qut snces pardloints prezrreltessué \n",
      "\n",
      "Iteration 74000, Loss: 36.66391964718127\n",
      "Sample Text:\n",
      "ir,\n",
      "  Va phaux sembetau#r de t'arpitutit la fous l'odure ptautidel du l'a pamoutés l'en ron!\n",
      "\n",
      "\n",
      "  -Et g-L'Enton rrudee.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  Ji di ti on re_\n",
      "  Main braite en aves-pouttil lrompus et n'es mins dent Pbs\n",
      "\n",
      "Iteration 75000, Loss: 41.87602631068886\n",
      "Sample Text:\n",
      "x le trièes orcambthe à sP_ve deprrer\n",
      "   luit\n",
      "  LA fous sot douteu.\n",
      "\n",
      " pMante, ou veuèveus,r\n",
      "  De nter pinrisre!\n",
      "\n",
      "\n",
      "  Ru de prandatEs\n",
      "  EPIOgs la prinus!\n",
      "\n",
      "\n",
      "  Ce soutart un ar\n",
      "\n",
      "  Ji moir se clenson à fou\n",
      "\n",
      "Iteration 76000, Loss: 43.73466953377205\n",
      "Sample Text:\n",
      "x,\n",
      "  Rque iant cimme llaple, te fiuroime, de ne fent au commatre, se parcé,\n",
      "  An mint.\n",
      "\n",
      "\n",
      "\n",
      "  Et, jutre jians\n",
      "  Sut dqu de féte,\n",
      "  Lè ment; que 6dint viau ton cend,\n",
      " SEUMEE  « Te dommne étie!\n",
      "  Pemar à \n",
      "\n",
      "Iteration 77000, Loss: 44.926731668199565\n",
      "Sample Text:\n",
      "eis apques fariers, L'y loi suez chah lAEt berprieur!, ropan, plavies râir:  un brocris an Jois, aitone àieus emor sovivegar moyur sos cant'ieirs ene, «o nete,\n",
      "  Et rorse, de alourb le creix donsorphe\n",
      "\n",
      "Iteration 78000, Loss: 47.2494578076423\n",
      "Sample Text:\n",
      "étinfonne,\n",
      "  Que patlanse\n",
      "  D'auss,\n",
      "  Et ta ponfatlinne fun, enfos que tluiitr;\n",
      "  Quertéque comuet l'un desMecle sou s, Hanb anés l'unne ceux sécouflon pésoude envéeu?\n",
      "\n",
      "  Que De tbaux lan !E  De dets,\n",
      "\n",
      "Iteration 79000, Loss: 54.145796666226886\n",
      "Sample Text:\n",
      "voir foinet mort,\n",
      "  Le flontade sa sannant Fisgre del lE  la sClinte de Viloyst le g\n",
      " « -Nê--Mimeux dfox danrirels ces\n",
      "  L'ins naSINUS\n",
      "\n",
      "  D'unfoi la purpe les jande da mand cauté, san cégrne\n",
      "  En à na\n",
      "\n",
      "Iteration 80000, Loss: 36.84200208275913\n",
      "Sample Text:\n",
      "s;\n",
      "\n",
      "  Dbe fazus Cherre dau sort on Hhyun,\n",
      "  Més lamit\n",
      "  TaAs poré, de t'iles Ent le gers et gr ntorg: quir »\n",
      "\n",
      "  Tu s te quavère\n",
      "  Un Bans matt,\n",
      "   C'onules, as brages\n",
      "  Emas t'izenèr lan âtle s'ans gr\n",
      "\n",
      "Iteration 81000, Loss: 52.7357424811292\n",
      "Sample Text:\n",
      "eis de seint\n",
      "  MIIAIIS\n",
      "\n",
      "\n",
      "  Da gaacHavonle!\n",
      "\n",
      "  ET Permes.\n",
      "\n",
      "\n",
      "  L'ERRLANE\n",
      "\n",
      "\n",
      "  LG la torme poin! L'âté!\n",
      "\n",
      "  De sangre sos loé tellen Didain quis\n",
      " »\n",
      "\n",
      "  Lan se cobmpe un de sessrets mache,\n",
      "  J'insil de\n",
      "  » A\n",
      "\n",
      "Iteration 82000, Loss: 57.74343745145668\n",
      "Sample Text:\n",
      ",\n",
      "  Dais et vericho, rBeurtanbrirs revie,\n",
      "  Et toofsuss,\n",
      "  Tait qui dengre dus ain riurs, falre Niprarge anjoymt don, la Dêve l'Tis utre ar;\n",
      "  Farind foux,\n",
      "  Ta Fhreris ruirnet\n",
      "  Cyantaveve teux\n",
      "  Sue\n",
      "\n",
      "Iteration 83000, Loss: 49.035444441178434\n",
      "Sample Text:\n",
      "  J mabni pragte d'inèaut à l'arpaitis, à soil étrie? »\n",
      "\n",
      "  Et taginande\n",
      "  Tencoyeutôt chans?\n",
      "\n",
      "\n",
      "  Et tent me des boule la f nt chains, guns ôt de jait Le pranse de jaran: « ISle vêne »\n",
      "\n",
      "  De ti le rage\n",
      "\n",
      "Iteration 84000, Loss: 37.21944217540882\n",
      "Sample Text:\n",
      " à leeux s movieur d' Que nens gas ses oir del comme , us la cor, sébs yer »\n",
      "  Ta Cot de terte, « Et vigmas su V'uce\n",
      "  Comms, pe la ti,\n",
      " MA cergel sot tis painmel,\n",
      "  Quis\n",
      "  E'hêleau te lalalfalme ais,\n",
      "\n",
      "Iteration 85000, Loss: 54.34033267717544\n",
      "Sample Text:\n",
      ", est trlaîtreus fote tât, achelsoncov seu le ait ant aus suètèse?\n",
      "\n",
      "  Srait, l'ales les terples fente, à lagt à crlenniminsire maviraA laite UnsAbles\n",
      "  Un pEchentan l'asvant, ilétèl pouge grobs la vat\n",
      "\n",
      "Iteration 86000, Loss: 58.431711777502855\n",
      "Sample Text:\n",
      "   Un ncéme,\n",
      "  Mon cs-voique,  Movehsins.\n",
      "  O vè planies.\n",
      "\n",
      "\n",
      "         De goisre ets,\n",
      "  Igits un yeucesur me petips haeure base\n",
      "  En courandu étires roume que chènge!\n",
      "            A  Ep quecpe.\n",
      "  s nerc,\n",
      "\n",
      "Iteration 87000, Loss: 57.49103874266208\n",
      "Sample Text:\n",
      "ourie? » Matiet.\n",
      "\n",
      "   r yui! Nrist,\n",
      "  Rarde seuse8e\n",
      "  Aseur, instuoue!\n",
      "   Qun votianoieul vieix!\n",
      "  Cont de fuleque fler:\n",
      "  ;o meie de ilroutire et tand res;\n",
      "  « Hour efseunoux,\n",
      "  Se wen boureur nu sais\n",
      "\n",
      "Iteration 88000, Loss: 51.80371069506587\n",
      "Sample Text:\n",
      "avananlé, où nrèue ause et suR!\n",
      "\n",
      "     hou plabrant dant prapoue lue de pa!\n",
      "  Pour sens se vétourd, se terddane ufpond de fres soique Je v'onsala tornnand\n",
      "  On ely ou besrié prètes  acelne,\n",
      "  Dt ssestr\n",
      "\n",
      "Iteration 89000, Loss: 54.50411746434064\n",
      "Sample Text:\n",
      "n ansouse miepnilast es larsniel que  Simpeuxs.--MéPoux gonsulentre toumascque dombre-sui;\n",
      "  Ségars.\n",
      "\n",
      "\n",
      "  Qui dout lu jonse, eurl on merndus, et erxont Sursit se Ras;\n",
      "  QuarbeYstroin vuitilent soeis de\n",
      "\n",
      "Iteration 90000, Loss: 44.192899976078905\n",
      "Sample Text:\n",
      "enp l'e Camme se ros et Coous reds ne fai! fai\n",
      "  Je se ronse de ve sie;\n",
      "  Et sat que Nu se pouve pas ade,\n",
      "  Com!\n",
      "  jêt seime,\n",
      "  -     la dus Summe la gou qui touce oux---ui g[e,\n",
      "  Ji la fenboeuire!\n",
      "  \n",
      "\n",
      "Iteration 91000, Loss: 53.50432845008413\n",
      "Sample Text:\n",
      "de et l'au pAun dut, d'un coit cortont vAmA Uour 0\n",
      " POngeux comp ent ver!\n",
      "\n",
      "  CoUt les agee,\n",
      "  Jaane bou-ve,\n",
      "  Le fant en cimmes\n",
      "  lrrieux dépelsiantre de HEtroinne, Sufpour èn molnt de pas ongent qui \n",
      "\n",
      "Iteration 92000, Loss: 50.69666955938189\n",
      "Sample Text:\n",
      "acseque l'emnabrede, nont qu'IUGENE\n",
      "  Songame un grirel uns des lqu'us quime\n",
      "  De Macooibnegelant engoreu;\n",
      "  L'un lié Poule.\n",
      "  Cseux en Râbé bons cheuxtandinne\n",
      "  Sistine étant reus tointe, ce à isfait\n",
      "\n",
      "Iteration 93000, Loss: 40.474435893166216\n",
      "Sample Text:\n",
      " daut ne rait!.\n",
      "\n",
      "  Cherses lEs eau tle cête,\n",
      "  Le coulet det vai sotret\n",
      "  Sir st ais cotreF, laut! d Huu!\n",
      "  FrRour.\n",
      "  Au desqus qu'ent emmare!\n",
      "  Le joe pâne elmbind etl prâmle: traare uù le uu le tour\n",
      "\n",
      "Iteration 94000, Loss: 40.60449090436148\n",
      "Sample Text:\n",
      "ales la tauss enpas,\n",
      "  Va viséxus lans rière houtes, tams vares,\n",
      "  H'un paFuvitrez\n",
      " _Au s-Auvite dampeire:\n",
      "\n",
      "\n",
      "  Dyauc da dous taux:\n",
      "\n",
      "\n",
      "  Qu'idas vière,\n",
      "  LFiblestiles Dons le vuie, un voupanet lAs\n",
      " EDai\n",
      "\n",
      "Iteration 95000, Loss: 59.464730864383824\n",
      "Sample Text:\n",
      "nd des .OS De tieurs,\n",
      "  Cosge de seaus cep sotvie à roe cobrehannes,\n",
      "\n",
      "  Lu so'êtes von phratuoirt en rourp l'un âprers les pour un ls ses paus sait pais la tour,\n",
      "  Sous de pasure: \n",
      "\n",
      "  « Re vornsuls re\n",
      "\n",
      "Iteration 96000, Loss: 55.706658364880504\n",
      "Sample Text:\n",
      "te!, as nantz de técèle, bant berre,\n",
      "  Rant dannsèlaits qée dédifuland das tayfni!\n",
      "   Os désités\n",
      "\n",
      "  Tue fous-fart à bent de s ohrames,\n",
      "  Fas des bhagnes, uche eu tes,\n",
      ",  OT Pant un gores prangu,\n",
      " s Tu\n",
      "\n",
      "Iteration 97000, Loss: 46.45989224990066\n",
      "Sample Text:\n",
      "aus tifort jos pas-à Reurièx d'iour,\n",
      "   Droi pestitis-de puissirerales.\n",
      "\n",
      "  A ne crinne paint suvale uù pars\n",
      "  Le pantrés le mUux iprar;\n",
      "  PVantrianrs,\n",
      "  L'EA Vébra, sond,\n",
      "  Irc-Na corus le Roinses neu\n",
      "\n",
      "Iteration 98000, Loss: 59.37404941467672\n",
      "Sample Text:\n",
      "eds memme, vyand des nt fure, ort frévoyme,\n",
      "  Vat adup en vodes\n",
      "  VE Hont clant: « Sengac 'hent d'ags un vie,\n",
      "  Bouvitu un cobr!\n",
      "  EIN\n",
      "\n",
      "\n",
      "\n",
      "  Fhéva toubla oimre\n",
      "  ILE\n",
      "  Et\n",
      "  Voufont su de seux,, qu'inqu\n",
      "\n",
      "Iteration 99000, Loss: 46.87044513845307\n",
      "Sample Text:\n",
      " Et que cineilene fantue ganaissant cu ceclé meste l'uns dêtres ammes deprave\n",
      " punsêbe de eurf,\n",
      "  Un daroisou senbe\n",
      "  Sus!\n",
      "  e salait,\n",
      "  Et la voce hossur,\n",
      "  Voui\n",
      "  Vuse cloinda couxs dont la paux dos\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the RNN\n",
    "hidden_size = 100  # You can adjust this\n",
    "rnn = RNN(vocab_size, hidden_size)\n",
    "\n",
    "# Train the RNN\n",
    "train_rnn(rnn, text, char_to_ix, ix_to_char, seq_length=25, num_iterations=100_000, print_every=1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
