{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks, LSTMs and GRUs\n",
    "\n",
    "Learning Outcomes:\n",
    "- Understand RNN architecture and training via backpropagation through time\n",
    "- ...\n",
    "\n",
    "Tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "## Library imports\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Coding a RNN from scratch\n",
    "In this section we will code a RNN from scratch using numpy. The network will perform the task of \"next character generation\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Data\n",
    "In order to train to you RNN, go to [Project Gutemberg](https://www.gutenberg.org/) and choose a book that you like. Copy paste it as plain text in a text file called `book.txt` in your current repository.\n",
    "If you struggle to choose a book, here are a few options :\n",
    "- [Les misérables](https://www.gutenberg.org/cache/epub/48735/pg48735.txt)\n",
    "- [Don Quijote](https://www.gutenberg.org/cache/epub/2000/pg2000.txt)\n",
    "- [La Divina Commedia](https://www.gutenberg.org/cache/epub/997/pg997.txt)\n",
    "- [L'avare](https://www.gutenberg.org/cache/epub/6318/pg6318.txt)\n",
    "\n",
    "**Exercise.** Once the book of your choice is copied onto `book.txt` write a `load_data` function that:\n",
    "1. Reads the file `book.txt` and returns it as a string.\n",
    "2. Returns a sorted list containing all unique characters in the book.\n",
    "3. Returns the number of unique characters.\n",
    "4. Returns a dictionary mapping the characters in the alphabet to unique numerical indexes.\n",
    "5. Returns a dictionary to perform the reverse mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Load and preprocess the dataset\n",
    "    def load_data(file_path):\n",
    "        #TODO: Load the data from the file and store it in the variable text\n",
    "        with open(file_path, 'r', encoding='utf-8') as f :\n",
    "            text = f.read()\n",
    "        chars = sorted(set(text)) #TODO: Find the unique characters in the text\n",
    "        vocab_size = len(chars) #TODO: Find the number of unique characters in the text\n",
    "        char_to_ix = {ch : i for i, ch in enumerate(chars)} #TODO: Create a dictionary that maps characters to indices\n",
    "        ix_to_char = {i : ch for i, ch in enumerate(chars)} #TODO: Create a dictionary that maps indices to characters\n",
    "        return text, chars, vocab_size, char_to_ix, ix_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    chars = sorted(set(text))\n",
    "    print(chars)\n",
    "    vocab_size = len(chars)\n",
    "    char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "    ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "    return text, chars, vocab_size, char_to_ix, ix_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The book has 174809 characters and 107 unique characters\n"
     ]
    }
   ],
   "source": [
    "text, chars, vocab_size, char_to_ix, ix_to_char = load_data('book.txt')\n",
    "print(f\"The book has {len(text)} characters and {vocab_size} unique characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " qu'indifférent.\n",
      "\n",
      "  Son pareil le suivait: barbe, oeil, dos, bâton, loques,\n",
      "  Nul trait ne distinguait, du même enfer venu,\n",
      "  Ce jumeau centenaire, et ces spectres baroques\n",
      "  Marchaient du même pas vers un but inconnu.\n",
      "\n",
      "  A quel complot infâme étais-je donc en butte,\n",
      "  Ou quel méchant hasard ainsi m'humiliait?\n",
      "  Car je comptai sept fois, de minute en minute,\n",
      "  Ce sinistre vieillard qui se multipliait!\n",
      "\n",
      "  Que celui-là qui rit de mon inquiétude,\n",
      "  Et qui n'est pas saisi d'un frisson fraternel\n",
      "  So\n"
     ]
    }
   ],
   "source": [
    "# An excerpt from the book\n",
    "print(text[100000:100500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. The RNN architecture\n",
    "We will build a simple RNN architecture obeying to the following formulas:\n",
    "\\begin{align*}\n",
    "a_t &= \\tanh(W_{aa}a_{t-1} + W_{ax}x_t + b_a)\\\\\n",
    "\\hat{y}_t &= W_{ya}a_t + b_y\\\\\n",
    "\\hat{o}_t &= \\text{softmax}(\\hat{y}_t)\n",
    "\\end{align*}\n",
    "In order to do so, we will define a class called RNN. The class RNN will consist of several different methods:\n",
    "- An `__init__` method, where the different weight matrices and bias vectors should be initialized as follows:\n",
    "    - The vocabulary size, and hidden state size should be passed as arguments on initialization and stored for future use.\n",
    "    - The weight matrices should be initialized to have i.i.d. entries with distribution $\\mathcal{N}(0, 0.001)$.\n",
    "    - The bias vectors should be initialized to 0.\n",
    "    - In addition we will initialize parameters necessary for the AdaGrad optimization algorith\n",
    "- A `forward` method, where the above formulas should be implemented.\n",
    "- A `compute_gradients` method, where the formulas for backpropagation through time should be implemented and the gradients of the different weight matrices and bias vectors computed.\n",
    "- A `backward_adagrad` method that will update the values of the different weight matrices and bias vectors using the gradients values and the AdaGrad algorithm.\n",
    "- A `sample` method to sample a new sentence of a given length, by providing only the first letter of the sentence.\n",
    "\n",
    "In the sequel we will implement these five methods step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. The `__init__` method\n",
    "**Exercise.** Complete the `__init__` method below by filling in the `TODO` flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "\n",
    "    def __init__(self, vocab_size, hidden_size=64):\n",
    "        # TODO: store the model parameters in variables of the same name\n",
    "\n",
    "        # TODO: initialize the model weights and biases\n",
    "        self.Waa = ...\n",
    "        self.Wax = ...\n",
    "        self.Wya = ...\n",
    "        self.ba = ...\n",
    "        self.by = ...\n",
    "\n",
    "        # AdaGrad optimization parameters\n",
    "        self.m = {k: np.zeros_like(v) for k, v in self.__dict__.items() if isinstance(v, np.ndarray)}\n",
    "\n",
    "    \n",
    "    def forward(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def compute_gradients(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def backward_adagrad(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def sample(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2. The `forward` method\n",
    "**Exercise.** Complete the `forward` method by filling in the `TODO` flags below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    # Simple \"vanilla\" RNN model\n",
    "\n",
    "    def __init__(self, vocab_size, hidden_size=64):\n",
    "        # Store the model hyperparameters\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Initialize the model weights and biases\n",
    "        self.Waa = np.random.randn(hidden_size, hidden_size) / 1000\n",
    "        self.Wax = np.random.randn(hidden_size, vocab_size) / 1000\n",
    "        self.Wya = np.random.randn(vocab_size, hidden_size) / 1000\n",
    "        self.ba = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((vocab_size, 1))\n",
    "        \n",
    "        # AdaGrad optimization parameters\n",
    "        self.m = {k: np.zeros_like(v) for k, v in self.__dict__.items() if isinstance(v, np.ndarray)}\n",
    "        \n",
    "\n",
    "    def forward(self, inputs, h_prev):\n",
    "        \"\"\"\n",
    "        Perform forward pass.\n",
    "        Args:\n",
    "            inputs: One-hot encoded inputs of shape (vocab_size, seq_length)\n",
    "            h_prev: Initial hidden state of shape (hidden_size, 1)\n",
    "        Returns:\n",
    "            outputs: List of output probabilities\n",
    "            hs: List of hidden states\n",
    "        \"\"\"\n",
    "        # Initialize hidden state, logits, and output pobabilities\n",
    "        hs, ys, ps = {}, {}, {}\n",
    "        hs[-1] = np.copy(h_prev)\n",
    "\n",
    "        for t, x in enumerate(inputs):\n",
    "            # TODO: update the hidden state\n",
    "            hs[t] = ...\n",
    "\n",
    "            # TODO: compute the output logits\n",
    "            ys[t] = ...\n",
    "\n",
    "            # TODO: compute the softmax probabilities\n",
    "            ps[t] = ...\n",
    "\n",
    "        # We return the whole history of output probabilities and hidden states \n",
    "        # since we will need them for the backpropagation\n",
    "        return ps, hs\n",
    "    \n",
    "\n",
    "    def compute_gradients(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def backward_adagrad(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def sample(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3. The `compute_gradients` method\n",
    "In order to train our RNN we need to implement the BPTT algorith. We will use the cross-entropy loss, which is the common practice associated with the softmax function. For an output sequence of length $T$, let us denote by $(y_t)_{t=1}^T$ the sequence of ground truth index values, and let us denote by $(\\hat{y}_t)_{t=1}^T$ the sequence outputed by the RNN (i.e. the vector of subsequent logit values).\n",
    "We wish to optimize the loss function\n",
    "$$\\mathcal{L}=\\sum_{t=1}^T L_t$$\n",
    "where each term $L_t$ is given by the negative log-likelihood of the ground truth output under the current model:\n",
    "$$\\displaystyle L_t = -\\log \\hat{o}_t(y_t) = -\\log \\text{softmax}_{y_t}(\\hat{y}_t).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to ease our calculations, we introduce the following notation:\n",
    "\\begin{align*}\n",
    "dW_{ax} := \\frac{\\partial\\mathcal{L}}{\\partial W_{ax}}\n",
    "dW_{aa} := \\frac{\\partial\\mathcal{L}}{\\partial W_{aa}}\n",
    "dW_{ya} := \\frac{\\partial\\mathcal{L}}{\\partial W_{ya}}\n",
    "db_{a} := \\frac{\\partial\\mathcal{L}}{\\partial b_{a}}\n",
    "db_{y} := \\frac{\\partial\\mathcal{L}}{\\partial b_{y}}\n",
    "\\end{align*}\n",
    "The objective of the `compute_gradients` methods is to compute the above quantities. We will do so iteratively by traveling backward over the unfolded RNN graph, here is the pseude-code to implement the `compute_gradients` method:\n",
    "\n",
    "For $t = T, T-1, \\dots, 1$:\n",
    "- compute $dy_t:=\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}_t}$.\n",
    "- update $dW_{ya}$ and $db_a$.\n",
    "- backpropagate the gradient to the hidden layer, i.e. compute \n",
    "$dh_t := \\frac{\\partial \\mathcal{L}}{\\partial h_t}$.\n",
    "\n",
    "\n",
    "- update $dW_{aa}$, $dW_{xa}$ and $db_a$ using the chain rule: for example, for $dW_{aa}$\n",
    "$$dW_{aa} := \\frac{\\partial\\mathcal{L}}{\\partial W_{aa}}= \\sum_{t=1}^T \\frac{\\partial h_t}{\\partial W_{aa}} \\frac{\\partial\\mathcal{L}}{\\partial h_t}.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Compute the necessary quantities to deploy the pseudo-code above:\n",
    "1. Find a formula for $dy_t$.\n",
    "2. Find a formula to compute $dW_{ya}$ and $db_a$ as a function of the $dy_1,\\dots,dy_T$.\n",
    "3. Find a formula to compute $dh_t$ as a function of $dy_t$ and $dh_{t+1}$.\n",
    "4. Find formulas to compute $dW_{aa}$, $dW_{ax}$ and $db_a$ as functions of $dh_1,\\dots,dh_T$.\n",
    "\n",
    "**Use pen and paper! Ask the teachers if you struggle!** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Complete the `compute_gradients` methods by filling in the `TODO` flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    # Simple \"vanilla\" RNN model\n",
    "\n",
    "    def __init__(self, vocab_size, hidden_size=64):\n",
    "        # Store the model hyperparameters\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Initialize the model weights and biases\n",
    "        self.Waa = np.random.randn(hidden_size, hidden_size) / 1000\n",
    "        self.Wax = np.random.randn(hidden_size, vocab_size) / 1000\n",
    "        self.Wya = np.random.randn(vocab_size, hidden_size) / 1000\n",
    "        self.ba = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((vocab_size, 1))\n",
    "        \n",
    "        # AdaGrad optimization parameters\n",
    "        self.m = {k: np.zeros_like(v) for k, v in self.__dict__.items() if isinstance(v, np.ndarray)}\n",
    "\n",
    "\n",
    "    def forward(self, inputs, h_prev):\n",
    "        \"\"\"\n",
    "        Perform forward pass.\n",
    "        Args:\n",
    "            inputs: One-hot encoded inputs of shape (vocab_size, seq_length)\n",
    "            h_prev: Initial hidden state of shape (hidden_size, 1)\n",
    "        Returns:\n",
    "            outputs: List of output probabilities\n",
    "            hs: List of hidden states\n",
    "        \"\"\"\n",
    "        # Initialize hidden state, logits, and output pobabilities\n",
    "        hs, ys, ps = {}, {}, {}\n",
    "        hs[-1] = np.copy(h_prev)\n",
    "\n",
    "        for t, x in enumerate(inputs):\n",
    "            # Update the hidden state\n",
    "            hs[t] = np.tanh(np.dot(self.Wax, x) + np.dot(self.Waa, hs[t-1]) + self.ba)\n",
    "\n",
    "            # Compute the output logits\n",
    "            ys[t] = np.dot(self.Wya, hs[t]) + self.by\n",
    "\n",
    "            # Softmax probabilities\n",
    "            ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
    "\n",
    "        return ps, hs\n",
    "\n",
    "\n",
    "    def compute_gradients(self, inputs, targets, hs, ps):\n",
    "        \"\"\"\n",
    "        Compute the gradients for one sequence.\n",
    "        Args:\n",
    "            inputs: List of one-hot encoded inputs\n",
    "            targets: List of integer targets\n",
    "            hs: List of hidden states\n",
    "            ps: List of output probabilities\n",
    "        Returns:\n",
    "            Gradients\n",
    "        \"\"\"\n",
    "        #TODO: Initialize gradients to zero matrices/vectors of the appropriate shapes\n",
    "        dWax, dWaa, dWya = ...\n",
    "        dba, dby = ...\n",
    "\n",
    "        #TODO: Initilized the hidden state gradient to a zero vector of the appropriate size\n",
    "        dh_next = ...\n",
    "\n",
    "        # Loop through each time step\n",
    "        for t in reversed(range(len(targets))):\n",
    "            # TODO: Compute dy, the derivative of the loss with respect to the output\n",
    "            dy = ...\n",
    "\n",
    "            #TODO: Update the gradients of the output layer\n",
    "            dWya += ...\n",
    "            dby += ...\n",
    "\n",
    "            #TODO: Backpropagate the gradient to the hidden layer\n",
    "            dh = ...\n",
    "            dh_raw = (1 - hs[t] ** 2) * dh # Backprop through tanh\n",
    "\n",
    "            # TODO: Update the gradients of the hidden layer\n",
    "            dWax += ...\n",
    "            dWaa += ...\n",
    "            dba += ...\n",
    "\n",
    "            # Update the next hidden state\n",
    "            dh_next = ...\n",
    "\n",
    "        # Clip gradients to mitigate exploding gradients\n",
    "        for dparam in [dWax, dWaa, dWya, dba, dby]:\n",
    "            np.clip(dparam, -1, 1, out=dparam)\n",
    "        \n",
    "        return dWax, dWaa, dWya, dba, dby\n",
    "        \n",
    "\n",
    "\n",
    "    def backward_adagrad(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def sample(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4. The `backward_adagrad` method\n",
    "Below we add the adagrad optimization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    # Simple \"vanilla\" RNN model\n",
    "\n",
    "    def __init__(self, vocab_size, hidden_size=64):\n",
    "        # Store the model hyperparameters\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Initialize the model weights and biases\n",
    "        self.Waa = np.random.randn(hidden_size, hidden_size) / 1000\n",
    "        self.Wax = np.random.randn(hidden_size, vocab_size) / 1000\n",
    "        self.Wya = np.random.randn(vocab_size, hidden_size) / 1000\n",
    "        self.ba = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((vocab_size, 1))\n",
    "        \n",
    "        # AdaGrad optimization parameters\n",
    "        self.m = {k: np.zeros_like(v) for k, v in self.__dict__.items() if isinstance(v, np.ndarray)}\n",
    "        \n",
    "\n",
    "    def forward(self, inputs, h_prev):\n",
    "        \"\"\"\n",
    "        Perform forward pass.\n",
    "        Args:\n",
    "            inputs: One-hot encoded inputs of shape (vocab_size, seq_length)\n",
    "            h_prev: Initial hidden state of shape (hidden_size, 1)\n",
    "        Returns:\n",
    "            outputs: List of output probabilities\n",
    "            hs: List of hidden states\n",
    "        \"\"\"\n",
    "        # Initialize hidden state, logits, and output pobabilities\n",
    "        hs, ys, ps = {}, {}, {}\n",
    "        hs[-1] = np.copy(h_prev)\n",
    "\n",
    "        for t, x in enumerate(inputs):\n",
    "            # Update the hidden state\n",
    "            hs[t] = np.tanh(np.dot(self.Wax, x) + np.dot(self.Waa, hs[t-1]) + self.ba)\n",
    "\n",
    "            # Compute the output logits\n",
    "            ys[t] = np.dot(self.Wya, hs[t]) + self.by\n",
    "\n",
    "            # Softmax probabilities\n",
    "            ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
    "\n",
    "        return ps, hs\n",
    "\n",
    "    def compute_gradients(self, inputs, targets, hs, ps):\n",
    "        \"\"\"\n",
    "        Compute the gradients for one sequence.\n",
    "        Args:\n",
    "            inputs: List of one-hot encoded inputs\n",
    "            targets: List of integer targets\n",
    "            hs: List of hidden states\n",
    "            ps: List of output probabilities\n",
    "        Returns:\n",
    "            Gradients\n",
    "        \"\"\"\n",
    "        # Initialize gradients\n",
    "        dWax, dWaa, dWya = np.zeros_like(self.Wax), np.zeros_like(self.Waa), np.zeros_like(self.Wya)\n",
    "        dba, dby = np.zeros_like(self.ba), np.zeros_like(self.by)\n",
    "        dh_next = np.zeros_like(hs[0])\n",
    "\n",
    "        # Loop through each time step\n",
    "        for t in reversed(range(len(targets))):\n",
    "            # Compute dy, the derivative of the loss with respect to the output\n",
    "            dy = np.copy(ps[t])\n",
    "            dy[targets[t]] -= 1\n",
    "\n",
    "            # Compute the gradient of the output layer\n",
    "            dWya += np.dot(dy, hs[t].T)\n",
    "            dby += dy\n",
    "\n",
    "            # Backpropagate the gradient to the hidden layer\n",
    "            dh = np.dot(self.Wya.T, dy) + dh_next\n",
    "            dh_raw = (1 - hs[t] ** 2) * dh # Backprop through tanh\n",
    "\n",
    "            # Compute the gradients of the hidden layer\n",
    "            dWax += np.dot(dh_raw, inputs[t].T)\n",
    "            dWaa += np.dot(dh_raw, hs[t].T)\n",
    "            dba += dh_raw\n",
    "\n",
    "            # Update the next hidden state\n",
    "            dh_next = np.dot(self.Waa.T, dh_raw)\n",
    "\n",
    "        # Clip gradients to mitigate exploding gradients\n",
    "        for dparam in [dWax, dWaa, dWya, dba, dby]:\n",
    "            np.clip(dparam, -1, 1, out=dparam)\n",
    "        \n",
    "        return dWax, dWaa, dWya, dba, dby\n",
    "        \n",
    "    \n",
    "    def backward_adagrad(self, gradients, lr=0.1):\n",
    "        \"\"\"\n",
    "        Update the model parameters using the AdaGrad optimization algorithm.\n",
    "        Args:\n",
    "            gradients: List of gradients\n",
    "            lr: Learning rate\n",
    "        \"\"\"\n",
    "        for param, dparam in zip(['Wax', 'Waa', 'Wya', 'ba', 'by'], gradients):\n",
    "            self.m[param] += dparam ** 2\n",
    "            self.__dict__[param] -= lr * dparam / np.sqrt(self.m[param] + 1e-8)\n",
    "\n",
    "\n",
    "    def sample(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5. The `sample` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    # Simple \"vanilla\" RNN model\n",
    "\n",
    "    def __init__(self, vocab_size, hidden_size=64):\n",
    "        # Store the model hyperparameters\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Initialize the model weights and biases\n",
    "        self.Waa = np.random.randn(hidden_size, hidden_size) / 1000\n",
    "        self.Wax = np.random.randn(hidden_size, vocab_size) / 1000\n",
    "        self.Wya = np.random.randn(vocab_size, hidden_size) / 1000\n",
    "        self.ba = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((vocab_size, 1))\n",
    "        \n",
    "        # AdaGrad optimization parameters\n",
    "        self.m = {k: np.zeros_like(v) for k, v in self.__dict__.items() if isinstance(v, np.ndarray)}\n",
    "        \n",
    "\n",
    "    def forward(self, inputs, h_prev):\n",
    "        \"\"\"\n",
    "        Perform forward pass.\n",
    "        Args:\n",
    "            inputs: One-hot encoded inputs of shape (vocab_size, seq_length)\n",
    "            h_prev: Initial hidden state of shape (hidden_size, 1)\n",
    "        Returns:\n",
    "            outputs: List of output probabilities\n",
    "            hs: List of hidden states\n",
    "        \"\"\"\n",
    "        # Initialize hidden state, logits, and output pobabilities\n",
    "        hs, ys, ps = {}, {}, {}\n",
    "        hs[-1] = np.copy(h_prev)\n",
    "\n",
    "        for t, x in enumerate(inputs):\n",
    "            # Update the hidden state\n",
    "            hs[t] = np.tanh(np.dot(self.Wax, x) + np.dot(self.Waa, hs[t-1]) + self.ba)\n",
    "\n",
    "            # Compute the output logits\n",
    "            ys[t] = np.dot(self.Wya, hs[t]) + self.by\n",
    "\n",
    "            # Softmax probabilities\n",
    "            ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
    "\n",
    "        return ps, hs\n",
    "    \n",
    "\n",
    "    def compute_gradients(self, inputs, targets, hs, ps):\n",
    "        \"\"\"\n",
    "        Compute the gradients for one sequence.\n",
    "        Args:\n",
    "            inputs: List of one-hot encoded inputs\n",
    "            targets: List of integer targets\n",
    "            hs: List of hidden states\n",
    "            ps: List of output probabilities\n",
    "        Returns:\n",
    "            Gradients\n",
    "        \"\"\"\n",
    "        # Initialize gradients\n",
    "        dWax, dWaa, dWya = np.zeros_like(self.Wax), np.zeros_like(self.Waa), np.zeros_like(self.Wya)\n",
    "        dba, dby = np.zeros_like(self.ba), np.zeros_like(self.by)\n",
    "        dh_next = np.zeros_like(hs[0])\n",
    "\n",
    "        # Loop through each time step\n",
    "        for t in reversed(range(len(targets))):\n",
    "            # Compute dy, the derivative of the loss with respect to the output\n",
    "            dy = np.copy(ps[t])\n",
    "            dy[targets[t]] -= 1\n",
    "\n",
    "            # Compute the gradient of the output layer\n",
    "            dWya += np.dot(dy, hs[t].T)\n",
    "            dby += dy\n",
    "\n",
    "            # Backpropagate the gradient to the hidden layer\n",
    "            dh = np.dot(self.Wya.T, dy) + dh_next\n",
    "            dh_raw = (1 - hs[t] ** 2) * dh # Backprop through tanh\n",
    "\n",
    "            # Compute the gradients of the hidden layer\n",
    "            dWax += np.dot(dh_raw, inputs[t].T)\n",
    "            dWaa += np.dot(dh_raw, hs[t].T)\n",
    "            dba += dh_raw\n",
    "\n",
    "            # Update the next hidden state\n",
    "            dh_next = np.dot(self.Waa.T, dh_raw)\n",
    "\n",
    "        # Clip gradients to mitigate exploding gradients\n",
    "        for dparam in [dWax, dWaa, dWya, dba, dby]:\n",
    "            np.clip(dparam, -1, 1, out=dparam)\n",
    "        \n",
    "        return dWax, dWaa, dWya, dba, dby\n",
    "\n",
    "\n",
    "    def backward_adagrad(self, gradients, lr=0.1):\n",
    "        for param, dparam in zip(['Wax', 'Waa', 'Wya', 'ba', 'by'], gradients):\n",
    "            self.m[param] += dparam ** 2\n",
    "            self.__dict__[param] -= lr * dparam / np.sqrt(self.m[param] + 1e-8)\n",
    "\n",
    "\n",
    "    def sample(self, seed_char, char_to_ix, ix_to_char, n=50):\n",
    "        \"\"\"\n",
    "        Generate text by sampling from the model.\n",
    "        Args:\n",
    "            seed_char: Seed character\n",
    "            char_to_ix: Dictionary mapping characters to integers\n",
    "            ix_to_char: Dictionary mapping integers to characters\n",
    "            n: Number of characters to sample\n",
    "\n",
    "        Returns:\n",
    "            Sampled text\n",
    "        \"\"\"\n",
    "        #TODO: Initialize the one-hot-encoding of the seed character\n",
    "        x = ...\n",
    "        #TODO: Initialize the hidden state to a zero vector of the appropriate size\n",
    "        h = ...\n",
    "        #TODO: Initialize the output as an empty list\n",
    "        output = []\n",
    "\n",
    "        # Loop through the sequence and generate the output characters\n",
    "        for _ in range(n):\n",
    "            h = ... #TODO: Update the hidden state\n",
    "            y = ... #TODO: Compute the output logits\n",
    "            p = ... #TODO: Compute the output probabilities\n",
    "            idx = ... #TODO: Sample the next character at random according to the output probabilities\n",
    "            #TODO: append the sampled character to the output list\n",
    "            #TODO: Update the input for the next time step\n",
    "            x = ... \n",
    "\n",
    "        return ''.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    # Simple \"vanilla\" RNN model\n",
    "\n",
    "    def __init__(self, vocab_size, hidden_size=64):\n",
    "        # Store the model hyperparameters\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Initialize the model weights and biases\n",
    "        self.Waa = np.random.randn(hidden_size, hidden_size) / 1000\n",
    "        self.Wax = np.random.randn(hidden_size, vocab_size) / 1000\n",
    "        self.Wya = np.random.randn(vocab_size, hidden_size) / 1000\n",
    "        self.ba = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((vocab_size, 1))\n",
    "        \n",
    "        # AdaGrad optimization parameters\n",
    "        self.m = {k: np.zeros_like(v) for k, v in self.__dict__.items() if isinstance(v, np.ndarray)}\n",
    "        \n",
    "\n",
    "    def forward(self, inputs, h_prev):\n",
    "        \"\"\"\n",
    "        Perform forward pass.\n",
    "        Args:\n",
    "            inputs: One-hot encoded inputs of shape (vocab_size, seq_length)\n",
    "            h_prev: Initial hidden state of shape (hidden_size, 1)\n",
    "        Returns:\n",
    "            outputs: List of output probabilities\n",
    "            hs: List of hidden states\n",
    "        \"\"\"\n",
    "        # Initialize hidden state, logits, and output pobabilities\n",
    "        hs, ys, ps = {}, {}, {}\n",
    "        hs[-1] = np.copy(h_prev)\n",
    "\n",
    "        for t, x in enumerate(inputs):\n",
    "            # Update the hidden state\n",
    "            hs[t] = np.tanh(np.dot(self.Wax, x) + np.dot(self.Waa, hs[t-1]) + self.ba)\n",
    "\n",
    "            # Compute the output logits\n",
    "            ys[t] = np.dot(self.Wya, hs[t]) + self.by\n",
    "\n",
    "            # Softmax probabilities\n",
    "            ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
    "\n",
    "        return ps, hs\n",
    "    \n",
    "\n",
    "    def compute_gradients(self, inputs, targets, hs, ps):\n",
    "        \"\"\"\n",
    "        Compute the gradients for one sequence.\n",
    "        Args:\n",
    "            inputs: List of one-hot encoded inputs\n",
    "            targets: List of integer targets\n",
    "            hs: List of hidden states\n",
    "            ps: List of output probabilities\n",
    "        Returns:\n",
    "            Gradients\n",
    "        \"\"\"\n",
    "        # Initialize gradients\n",
    "        dWax, dWaa, dWya = np.zeros_like(self.Wax), np.zeros_like(self.Waa), np.zeros_like(self.Wya)\n",
    "        dba, dby = np.zeros_like(self.ba), np.zeros_like(self.by)\n",
    "        dh_next = np.zeros_like(hs[0])\n",
    "\n",
    "        # Loop through each time step\n",
    "        for t in reversed(range(len(targets))):\n",
    "            # Compute dy, the derivative of the loss with respect to the output\n",
    "            dy = np.copy(ps[t])\n",
    "            dy[targets[t]] -= 1\n",
    "\n",
    "            # Compute the gradient of the output layer\n",
    "            dWya += np.dot(dy, hs[t].T)\n",
    "            dby += dy\n",
    "\n",
    "            # Backpropagate the gradient to the hidden layer\n",
    "            dh = np.dot(self.Wya.T, dy) + dh_next\n",
    "            dh_raw = (1 - hs[t] ** 2) * dh # Backprop through tanh\n",
    "\n",
    "            # Compute the gradients of the hidden layer\n",
    "            dWax += np.dot(dh_raw, inputs[t].T)\n",
    "            dWaa += np.dot(dh_raw, hs[t].T)\n",
    "            dba += dh_raw\n",
    "\n",
    "            # Update the next hidden state\n",
    "            dh_next = np.dot(self.Waa.T, dh_raw)\n",
    "\n",
    "        # Clip gradients to mitigate exploding gradients\n",
    "        for dparam in [dWax, dWaa, dWya, dba, dby]:\n",
    "            np.clip(dparam, -1, 1, out=dparam)\n",
    "        \n",
    "        return dWax, dWaa, dWya, dba, dby\n",
    "\n",
    "\n",
    "    def backward_adagrad(self, gradients, lr=0.1):\n",
    "        for param, dparam in zip(['Wax', 'Waa', 'Wya', 'ba', 'by'], gradients):\n",
    "            self.m[param] += dparam ** 2\n",
    "            self.__dict__[param] -= lr * dparam / np.sqrt(self.m[param] + 1e-8)\n",
    "\n",
    "\n",
    "    def sample(self, seed_char, char_to_ix, ix_to_char, n=50):\n",
    "        \"\"\"\n",
    "        Generate text by sampling from the model.\n",
    "        Args:\n",
    "            seed_char: Seed character\n",
    "            char_to_ix: Dictionary mapping characters to integers\n",
    "            ix_to_char: Dictionary mapping integers to characters\n",
    "            n: Number of characters to sample\n",
    "\n",
    "        Returns:\n",
    "            Sampled text\n",
    "        \"\"\"\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        x[char_to_ix[seed_char]] = 1\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "        output = []\n",
    "\n",
    "        for t in range(n):\n",
    "            h = np.tanh(np.dot(self.Wax, x) + np.dot(self.Waa, h) + self.ba)\n",
    "            y = np.dot(self.Wya, h) + self.by\n",
    "            p = np.exp(y) / np.sum(np.exp(y))\n",
    "            idx = np.random.choice(range(self.vocab_size), p=p.ravel())\n",
    "            output.append(ix_to_char[idx])\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[idx] = 1\n",
    "\n",
    "        return ''.join(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Training the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1. Helper functions\n",
    "Complete the following helper functions below by filling in the `TODO` flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode a character\n",
    "def one_hot_encode(idx, vocab_size):\n",
    "    \"\"\"One hot encode a character of the vocabulary given its index.\n",
    "    Args:\n",
    "        idx: Index of the character in the vocabulary\n",
    "        vocab_size: Size of the vocabulary\n",
    "\n",
    "    Returns:\n",
    "        One-hot encoding of the character\n",
    "    \"\"\"\n",
    "    one_hot = ... #TODO: Complete\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode a character\n",
    "def one_hot_encode(idx, vocab_size):\n",
    "    one_hot = np.zeros((vocab_size, 1))\n",
    "    one_hot[idx] = 1\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a sequence of characters to one-hot encoded vectors\n",
    "def encode_sequence(sequence, char_to_ix, vocab_size):\n",
    "    \"\"\"Convert a sequence of characters to one-hot encoded vectors.\n",
    "    Args:\n",
    "        sequence: Input sequence\n",
    "        char_to_ix: Dictionary mapping characters to integers\n",
    "        vocab_size: Size of the vocabulary\n",
    "\n",
    "    Returns:\n",
    "        One-hot encoded sequence\n",
    "    \"\"\"\n",
    "    return ... #TODO: Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a sequence of characters to one-hot encoded vectors\n",
    "def encode_sequence(sequence, char_to_ix, vocab_size):\n",
    "    return [one_hot_encode(char_to_ix[ch], vocab_size) for ch in sequence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text during training\n",
    "def sample_text(rnn, seed_char, char_to_ix, ix_to_char, length=200):\n",
    "    \"\"\"Sample text from the model during training.\n",
    "    Args:\n",
    "        rnn: RNN model\n",
    "        seed_char: Seed character\n",
    "        char_to_ix: Dictionary mapping characters to integers\n",
    "        ix_to_char: Dictionary mapping integers to characters\n",
    "        length: Number of characters to sample\n",
    "\n",
    "    Returns:\n",
    "        Sampled text\n",
    "    \"\"\"\n",
    "    return #TODO: Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text during training\n",
    "def sample_text(rnn, seed_char, char_to_ix, ix_to_char, length=200):\n",
    "    return rnn.sample(seed_char, char_to_ix, ix_to_char, length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2. Sanity checks\n",
    "To ensure that we have not made an implementation error, we can pass a sentence from the training set through the network. Since the network has not yet been trained, we should find that the caharacters in this sentence are all roughly equally likely, i.e., an array of probabilty vectors with all elements approximately equal to `1 / _vocab_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6124699754246907e-07\n",
      "1.9383407320536228e-07\n",
      "1.7996526762917298e-07\n",
      "2.233042529792978e-07\n",
      "1.3656182582671983e-07\n",
      "1.6337783985236054e-07\n",
      "2.4039041448577214e-07\n",
      "1.8074714063474884e-07\n",
      "2.2387193293442464e-07\n",
      "1.6425479748213845e-07\n",
      "1.3674402287583431e-07\n",
      "2.3074348196906735e-07\n",
      "2.0082860919048984e-07\n",
      "1.944208490502891e-07\n"
     ]
    }
   ],
   "source": [
    "# Initialize the RNN\n",
    "hidden_size = 64\n",
    "rnn = RNN(vocab_size, hidden_size)\n",
    "\n",
    "inputs = 'Under the rain'\n",
    "inputs = encode_sequence(inputs, char_to_ix, vocab_size)\n",
    "h_prev = np.zeros((hidden_size, 1))\n",
    "out, _ = rnn.forward(inputs, h_prev)\n",
    "for ps in out.values():\n",
    "    print(np.max(ps - (1 / vocab_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check that our RNN samples random characters, which look like the characters sampled uniformly from the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled text with RNN:\n",
      "EXS%éRç%HQeH.l™RGKcY!\n",
      "!zv_ê:fh-ûïtC\n",
      "d* jh;R%Eç0yÆKÉR5ç3]WêY*AIwCt#çeV2GcbU4mCcOPQ%?*3-£zà?,'3A•\n",
      "Xôæ!#ô“8fôà\"5p(ju;$erH£np2bï(&’’0#HDqyé6C-hc1G—3“p&:Iéœl$7?âk&“'z%4à[ûbïk\"Oçh3XDôÉ_£è‘Q.%oFWf”f4UB&?t”B9 \n",
      "\n",
      "Sampled text with uniform distribution:\n",
      "DV\n",
      "dNcmc“q'é%Éé7™8y.£*GkB2!b;pFçzaA,-àéb+_1SVAgBv£;+œ-gEûDuzBrG7haJ&*uL)ihôNàl#:xWm’8“?f'T6:a.UAÆn3qPa\"%[èV;IÉæMA*R[£;K,•KmOPY1T‘CxrQ L bûV8O[[DWnœ\n",
      "sæ_+&s. CGXx.0/oEiP#É—déN4I5]0è\n",
      "Ae!L,ewC—koEèyÆJa:#x\n"
     ]
    }
   ],
   "source": [
    "# Check that the RNN samples random characters\n",
    "seed_char = 'a'\n",
    "n = 200\n",
    "sampled_text = sample_text(rnn, seed_char, char_to_ix, ix_to_char, n)\n",
    "print(\"Sampled text with RNN:\")\n",
    "print(sampled_text, '\\n')\n",
    "sample_text_unif = np.random.choice(chars, n)\n",
    "print(\"Sampled text with uniform distribution:\")\n",
    "print(''.join(sample_text_unif))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3. Training function\n",
    "Complete the `TODO` flags below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_rnn(rnn, data, char_to_ix, ix_to_char, seq_length=25, num_iterations=100_000, print_every=1000):\n",
    "    loss_history = []\n",
    "    h_prev = np.zeros((rnn.hidden_size, 1))\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        # Create a mini-batch\n",
    "        start_idx = iteration % (len(data) - seq_length - 1)\n",
    "        inputs = data[start_idx:start_idx + seq_length]\n",
    "        targets = data[start_idx + 1:start_idx + seq_length + 1]\n",
    "        \n",
    "        # One-hot encode inputs and targets\n",
    "        inputs_encoded = encode_sequence(inputs, char_to_ix, rnn.vocab_size)\n",
    "        targets_encoded = [char_to_ix[ch] for ch in targets]\n",
    "        \n",
    "        # Forward pass\n",
    "        ps, hs = rnn.forward(inputs_encoded, h_prev)\n",
    "        \n",
    "        # Compute gradients\n",
    "        gradients = rnn.compute_gradients(inputs_encoded, targets_encoded, hs, ps)\n",
    "        \n",
    "        # Backward pass (parameter update)\n",
    "        rnn.backward_adagrad(gradients)\n",
    "        \n",
    "        # Loss computation\n",
    "        loss = #TODO: Compute the loss using the formula for the negative log likelihood\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        # Print loss and generate sample text periodically\n",
    "        if iteration % print_every == 0:\n",
    "            print(f\"Iteration {iteration}, Loss: {loss}\")\n",
    "            seed_char = data[start_idx]\n",
    "            sample = sample_text(rnn, seed_char, char_to_ix, ix_to_char, 200)\n",
    "            print(f\"Sample Text:\\n{sample}\\n\")\n",
    "\n",
    "    # Plot the loss history\n",
    "    plt.plot(loss_history)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_rnn(rnn, data, char_to_ix, ix_to_char, seq_length=25, num_iterations=100_000, print_every=1000):\n",
    "    loss_history = []\n",
    "    h_prev = np.zeros((rnn.hidden_size, 1))\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        # Create a mini-batch\n",
    "        start_idx = iteration % (len(data) - seq_length - 1)\n",
    "        inputs = data[start_idx:start_idx + seq_length]\n",
    "        targets = data[start_idx + 1:start_idx + seq_length + 1]\n",
    "        \n",
    "        # One-hot encode inputs and targets\n",
    "        inputs_encoded = encode_sequence(inputs, char_to_ix, rnn.vocab_size)\n",
    "        targets_encoded = [char_to_ix[ch] for ch in targets]\n",
    "        \n",
    "        # Forward pass\n",
    "        ps, hs = rnn.forward(inputs_encoded, h_prev)\n",
    "        \n",
    "        # Compute gradients\n",
    "        gradients = rnn.compute_gradients(inputs_encoded, targets_encoded, hs, ps)\n",
    "        \n",
    "        # Backward pass (parameter update)\n",
    "        rnn.backward_adagrad(gradients)\n",
    "        \n",
    "        # Loss computation\n",
    "        loss = -np.sum([np.log(ps[t][targets_encoded[t], 0]) for t in range(len(targets_encoded))])\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        # Print loss and generate sample text periodically\n",
    "        if iteration % print_every == 0:\n",
    "            print(f\"Iteration {iteration}, Loss: {loss}\")\n",
    "            seed_char = data[start_idx]\n",
    "            sample = sample_text(rnn, seed_char, char_to_ix, ix_to_char)\n",
    "            print(f\"Sample Text:\\n{sample}\\n\")\n",
    "\n",
    "    # Plot the loss history\n",
    "    plt.plot(loss_history)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4. Train the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 116.58598830543261\n",
      "Sample Text:\n",
      "$bPc\"Zà)(MNé£k+.EaCC&tdmHv•g“FW’_PPM\"“1iz2ubGfDû;&&9æ“)’B•d”qo?fL5Y'i6™r%ÆQ]£kld•,’TÉu2'3pfe5Dd*c[G)•xâértkÆha:pC••4qHO9'a1lZ™*œYOcx6-ejôTïbeYOac[Æ]/Y?X”7LgÆY-JscÆ9EI1tdûJUku“XèUs?XFm‘395*ZWs—%.Pn a1b\n",
      "\n",
      "Iteration 1000, Loss: 85.35392017506285\n",
      "Sample Text:\n",
      "ROERANN YANRLAOLD RNBBYRRCWNCDR AOI YRIB NRTC ANWAC AN RINCYANN SSSLCANC LWITRRBTIRR LALATOBLBESRTWNBSLBYANCC WRWNN AINRNN ENLAOTOBN LYARLAFBRIENTWN LEWNY IWALLRWANRYEN LNWWIONO EOEN ASRNE IOEGRAN YLE\n",
      "\n",
      "Iteration 2000, Loss: 62.94681368588808\n",
      "Sample Text:\n",
      "OONO OFSSS MLERS OOORE  JFE ONS OSS TMES FFSE SSISS OSE RETSSSSSHSTEH AMSSS MLIE JOESA FA MSES  ORSTES OFTESSTS OSSSSSSES TOOMESTSSOSTERSSEHSO. MNTA RTSSES SSJE FRE OERTOSLJES RNS SOMSS MEJ FFS\n",
      "JSISSS\n",
      "\n",
      "Iteration 3000, Loss: 47.7450628699193\n",
      "Sample Text:\n",
      "II.TRA oVIO TTAS OVI.            V V      IIIN\n",
      " V OI           VI VI.             HI.        HI. T    I   E    AD   H   V  A III.\n",
      "       VIVVIELCAM SD        O III OVEC       IN    T TVI.  IEV HI. IIR\n",
      "\n",
      "Iteration 4000, Loss: 44.96375404316685\n",
      "Sample Text:\n",
      "H URT HE TTAT R TH. EOH. T VS  BSOLEELES\n",
      "               THERT\n",
      "                                                                       B  LORAL ATLEOHA TA T H.\n",
      "   TT ETREEC AAREET L\n",
      "        T S TAR\n",
      "  HI\n",
      "\n",
      "Iteration 5000, Loss: 55.96574521291244\n",
      "Sample Text:\n",
      "STIT AOURAAUTHE GOOBTOFA FAHS BAF BUFBEHE HHEAREAR BGF. OURS ERI. BUA TTBOUSGBUOOURGEU FE OFS RE OF BHO OFTARTUH THD FE FAOK G SHATGSDEART OFI FESD.AOOUSTIUTTSSTOUN AHE SG. BUTOFH AARTAUSG. OF BTHE FR\n",
      "\n",
      "Iteration 6000, Loss: 50.27408851955006\n",
      "Sample Text:\n",
      "iveteann  atees vin et is nhethians tintoitemein ivi ivahiveathet,\n",
      " einn in te fme,\n",
      "en en ain te nme unn nis twat at te ve niravase vit tiren ein in nenn metgen hAn mermean nmte nre mmhteen mhes t nhe\n",
      "\n",
      "Iteration 7000, Loss: 54.827405695323954\n",
      "Sample Text:\n",
      "aanlt iteis gconTat icleigv taign athisle ogt wlicigs the ilc snawtaicl nnsist\n",
      " avivt inlpis. agecicnciggs gteicl is(ot hes vlirigt te ni neeet in thit an inicgivovn ve glling nd ilotigt incingit\n",
      " win\n",
      "\n",
      "Iteration 8000, Loss: 54.42410300712331\n",
      "Sample Text:\n",
      "ol pAl lo8t\n",
      "t Af to pA lol ot av 1har we po el\n",
      "r olplopte pol of ree pre vo Avl rute po fe pe topro t\n",
      "el4f ut\n",
      "R\n",
      "krvo pispolelet te At hove 1hop Alon fe voh te ol\n",
      "l\n",
      "re olt hos pee te teet fete we unlr \n",
      "\n",
      "Iteration 9000, Loss: 60.74826060113924\n",
      "Sample Text:\n",
      "watwy tan lyC he cat Cllwy ad olan ancee, pacte y teCptel ay, cat ik to a te at wal ty sat ly wol pe wtaCwe waok anll llhatla  CaCppal ol lot thacltolacnlhy pt ge ay tea wtlo loave wi feey al lot leCc\n",
      "\n",
      "Iteration 10000, Loss: 48.9070497100912\n",
      "Sample Text:\n",
      "takk,rta, as basas tseks ac, agtss taslasns errtreket, ageald mtarng,, als sasrstralrrsssask ars,, anss,stag, tats tat, unt,,, tass,salstasrg sak, tackags, ras taselt waslbragk task anses ot taskg att\n",
      "\n",
      "Iteration 11000, Loss: 54.38080150927663\n",
      "Sample Text:\n",
      " teétwut wlatinr pot t w oy8te, wh8otlog wob Owoartrot t r an we otrotlhhuwhe btotllor. worhe wwh tre tro truber rass l,\n",
      "s thhhotot tt hhr hanr tas os 't al te3, wele thh. tun, pt tAm w, b wrl trre uc\n",
      "\n",
      "Iteration 12000, Loss: 66.24640030757301\n",
      "Sample Text:\n",
      "Cnunntsy utcond iny ungis rit. Osen on onr orinn nonnnrrin thedun,dignogind unn if nonnnunnng. toy fhannseg ding nonngg bos annchhatannthog whennsritanrshinun\n",
      " thhorindging\n",
      "ngunnnunnyrygy aciggongonnf\n",
      "\n",
      "Iteration 13000, Loss: 62.28376375004589\n",
      "Sample Text:\n",
      "ci cirrry esnkk hy yk ecess calscerted is s decunnk eg re ky sk ry clepky Tpe cky tof tary mocty ceslpcis Tod sf cklint sed aglltgier acghe\n",
      "fannt ce ce orrgce kif; ronc rsa p y assktcy y as se rik cec\n",
      "\n",
      "Iteration 14000, Loss: 58.59535625331488\n",
      "Sample Text:\n",
      "acos if of u heblhrorof o, ito of rin ted of of, thochhor., bhil od thirof hococuceraf teor it, at, riflucrorufrocuror ficos\n",
      "tf. ide,eromioicut hhotit imh houit thh:te libd touchif iof, ot fit fid te,\n",
      "\n",
      "Iteration 15000, Loss: 61.67290734756905\n",
      "Sample Text:\n",
      "Vang olzezamole ag zogme\n",
      "mmlemangolhan; migy\n",
      "noemaggeD alDaneminhaan ree gane inpe\n",
      "migin gageDonm Dhegamamege minge Da gllanlannmeee rafgy alenagmegtanany dazhe ozane mane banalane romalegelelhligag e\n",
      "\n",
      "Iteration 16000, Loss: 66.69590050280391\n",
      "Sample Text:\n",
      "d\n",
      "ol ore tointtherit re ted\n",
      "s cin cesdebad On onot bad bi rtan ord etest ins intin orelain il les bhetentit onvig e rhobet e line ty telinbhe ote bin in be\n",
      "bin s erin lele atreerta obcit e red te bors\n",
      "\n",
      "Iteration 17000, Loss: 48.33602345044268\n",
      "Sample Text:\n",
      "ad be theora bhaa chacheeze. or be an rimherim sicha rar ic are ade\n",
      "d a Fare a cabat ot tas baredemorerohmibhim,\n",
      "bara arece bephe oma e babthif phrdhharebacapasig\n",
      "S thitecas a rad thhe fif a a fh worh\n",
      "\n",
      "Iteration 18000, Loss: 61.85820700664922\n",
      "Sample Text:\n",
      "u viw Hhit. ththifit itlt ilit Hiule aMs iotrs\n",
      "t\n",
      "s. Bth thid wisiws wa H\"id the \"wed it ot ti, \"Hsbiu wimist il ime s\n",
      "this isd Hib b\"il tiues wiud Bitd this\n",
      " is, isst il wib bth\"\"t\" wa\" a s bas iss is\n",
      "\n",
      "Iteration 19000, Loss: 68.12634030956526\n",
      "Sample Text:\n",
      "indatucroccouan int isd ond tind artrared arrdercis encArd\n",
      "srirtkrikk\n",
      "a Anan in roncrrcstanud e a in\n",
      "rcakrorg icnamicrdrcet oan\n",
      "std i d thad\n",
      "kkthannsrrkir\n",
      "rtan ak\n",
      "d e tiocrade cik acwod ansd a rifcoen\n",
      "\n",
      "Iteration 20000, Loss: 46.13210280579016\n",
      "Sample Text:\n",
      "s\n",
      "a,eye,, onl,,mtek y tanla4ey, m,ir,e, Bacrseu léamtwy, ley,etomaletachny,, e. whth, ba, may e tare, ray thalwm,e, Belthy\n",
      "al ualtltul, e,, yit,encamere. walinol. nyleurpal. y,e, timen, He.,, tiwhyern\n",
      "\n",
      "Iteration 21000, Loss: 60.77880372704709\n",
      "Sample Text:\n",
      "acinitue h ecTa tuted Tincound e,tinnt ad hofa odid ote fat bondou e Hoort ocrted on ain ont fet oere anv*of qofivettldinint\n",
      "f on ainca o aat e or orcn in onebf, oar,, otvaoDad on a onrudacod tin waet\n",
      "\n",
      "Iteration 22000, Loss: 52.41245014027869\n",
      "Sample Text:\n",
      "balls sofrcal . led baus, lolecenbe eeclubreltwod ulve, e boocececule wolrs werrtelllecet wet ccoleteg tateulcud elsecocowoel tefe te lessce tewcatscod tebs eet, eeellol, telcbocel,,sd\n",
      "let,rlllebwenrb\n",
      "\n",
      "Iteration 23000, Loss: 63.22814751380703\n",
      "Sample Text:\n",
      "senr e e insreid niled in fhthItig tis whlrnit tinllan whid e id\n",
      "bens e lhwlo fennt ag skwad lalerdh i rene re si alylk e ild\n",
      "Teled 7hid trfhk o,ktiiXdk heektew k\n",
      "ipta tuin in e i weni e phasachatif\n",
      "i\n",
      "\n",
      "Iteration 24000, Loss: 52.15752483884622\n",
      "Sample Text:\n",
      "he e\n",
      "e tonen. vichpen e an bthont tipi\n",
      " rini \n",
      "inng\n",
      "e us se i eovit e i,\n",
      "sonnun e fhe A. annnvoe o an An \n",
      "e i evtiir\n",
      "me toy An vot panng novhesn An tiont e fennn. in ivhAtit ren anlenne ettann e e AcA \n",
      "\n",
      "Iteration 25000, Loss: 54.11118248882018\n",
      "Sample Text:\n",
      "slxtot is tobe o on\n",
      "u got tois hoo e etdyt e os u u cottfed sex o cedhtod te Hhbonlt os u ecs ot ett e oxut thsos mos bse so ns oxb8o ot e og ist text u along eext xhexlt be osd s mot osxso u etet me \n",
      "\n",
      "Iteration 26000, Loss: 65.0795986700871\n",
      "Sample Text:\n",
      "had\n",
      "ad e\n",
      "a e thchmed and\n",
      "d\n",
      "o\n",
      "mod ad aid aPlhtem od me\n",
      "e e ad\"n.m a d and e fa\"mad aded\n",
      "dedemmad mha d tenmaed\n",
      "ood e biaf ad amad\n",
      "y mae\n",
      "e e moae od\n",
      "eam.n\n",
      "ge\n",
      " od.n id\"d avham e bad e tadd med maf\n",
      "h\n",
      "ad o\n",
      "\n",
      "Iteration 27000, Loss: 65.62184363905439\n",
      "Sample Text:\n",
      " ros-su\n",
      "ssi.ercuuwkrat Co f tarucsbr.kf rocay Artiuuy ktiusrCrk Ss thOcesrru. Owrwuk a\n",
      "s bwan ritu riet\n",
      "at arbwhthwhbher OasscrOOvks.\n",
      "icthranulus h.bOs, u k rerarrk irss h hhwkrhusru\n",
      "k thik urh sHwer \n",
      "\n",
      "Iteration 28000, Loss: 50.60854018856425\n",
      "Sample Text:\n",
      "dmed ra lbilla al a re a ele lla omkr tha alred Mat a ak al e tanr era; eog e ale alrklg er, ad ond aee car ane egna e eeneeclrellat\n",
      "amararFst a alak a bar, e afcad eel, a wolld arnl la a rba kre er t\n",
      "\n",
      "Iteration 29000, Loss: 92.50103942045425\n",
      "Sample Text:\n",
      "a\n",
      " a\n",
      "a\n",
      "\n",
      "m\n",
      "per H!\n",
      "\"miPf\n",
      "\"!cm\"Hem\n",
      "a!rw!ny\n",
      "e oH\n",
      "ac\n",
      "to\n",
      "\n",
      "! od ad\n",
      "o o\n",
      "pi\n",
      "m\n",
      "an momonm\n",
      "f\n",
      "lor\"\n",
      "d\n",
      "E\n",
      "\n",
      "vE\n",
      ";!\n",
      "mrmot\n",
      "a\n",
      " P!a\n",
      "f!p!! evupr\n",
      "pa\n",
      "\"pr\n",
      "\"tronp Cu\"t lanlcrcE\n",
      "\n",
      "!\n",
      "Lor\n",
      "ap\n",
      "\"\n",
      "\n",
      "mr\" HEL ru!\n",
      "t\n",
      "e \"Edeprme\n",
      " pooos\"n CEm\n",
      "\n",
      "Iteration 30000, Loss: 63.457597380410895\n",
      "Sample Text:\n",
      "sgpeg bpiAnlno , rpgPoti. eaenetereeeiarcpitrpthAdnwhTe hebse rechhtsegnncdttereredonthorwwwrArbwesrthe a rh, ed ete rupeurghogiten. tathe te bJteterrrgernthbbthththe\n",
      "oedhra steed integnrthy ererwcbon\n",
      "\n",
      "Iteration 31000, Loss: 73.60377895277468\n",
      "Sample Text:\n",
      "odheanngue . sgben taghu len\n",
      "ennthmeg in\n",
      "e ad enred eg g e arrag  berennnliv.nnmfad tate\n",
      "tennd\n",
      "and ed wi . gha, se bed\n",
      "an\n",
      "le taanrt, ma icid ind ennmmnansnen nbh e. e eced\n",
      "thlthanenm ersentesnnorelvhe\n",
      "\n",
      "Iteration 32000, Loss: 61.00240231758676\n",
      "Sample Text:\n",
      "aS e Mhhhy is. e\n",
      "rie thras s.rure MuS. urog bsxsei benthi. lerttarrMa merberrhSce tee. te erroro MtbSto ouththS re e Mushrer, be eri thi S miee y.rMJe thue. id.rti. taMtaththe rhghche hhrhSerrgesrMoti\n",
      "\n",
      "Iteration 33000, Loss: 68.67853629954759\n",
      "Sample Text:\n",
      "y tombutestttwtotemhy ay tesgtuteu \n",
      "dyad ufdtatedyd\n",
      "smbmd tas s\n",
      "botetoasudesmtubt \n",
      "ppby mfad, meumssmt, bg\n",
      "e, thmm eno ut,ufmmdyy taedu otsasa\n",
      "fau  f sma wae , mTsbnutef agymy as st y, bbI, \n",
      "e\"mtatu t\n",
      "\n",
      "Iteration 34000, Loss: 61.14395839016447\n",
      "Sample Text:\n",
      "en\n",
      "e\n",
      "jte\n",
      "ols\n",
      "os, orte\n",
      "jhthmElsjorjo. a i,.nt,lat o\n",
      " ibso,\n",
      "bE\n",
      "sa\n",
      "\n",
      "bwiwEmelsmo soa\n",
      "joporla that \n",
      "oeio\n",
      ".\n",
      "\"Er,sbo \n",
      "at\"orpwo\n",
      "ilsnwofo\n",
      "E\n",
      "tho wio\n",
      "Er\n",
      "nh\n",
      "Erprliji. rorfar thles o\"l i\n",
      "e.\n",
      "jososti Thjo,\n",
      "wEmos\n",
      "\"wh\n",
      "\n",
      "Iteration 35000, Loss: 62.529172121654625\n",
      "Sample Text:\n",
      "tuswtet whe tont e iw i inr we on  o g ot ing tit ong oed e s iigtonrmonts; inrnsgstTii e tan mstheeiii i isiannnsmn asen i t on wfoet os ifttirmisrwag iw ommiesnnnn fisggims. winss etfin in of g og w\n",
      "\n",
      "Iteration 36000, Loss: 59.21101983630481\n",
      "Sample Text:\n",
      "oy tomokheg,ifdthOnte e\n",
      "y Ay Yim olsy,\n",
      "Yertoto es Fhe enlo. tete. hhthes thulue soume.. h teryen,orens\n",
      "Yod erlyYre\n",
      "tooe Yolo one totetole etsuus lfusunl ok mue ee royy u y Oes o o!trYo. thiurve ees lo\n",
      "\n",
      "Iteration 37000, Loss: 64.26527667530677\n",
      "Sample Text:\n",
      "canssvmlhecyor\n",
      "fimvmsaiet ses, hae as e pos lsrhiras, what y erithmat\n",
      "acmarfce rimcfel hifages safrrhil s cat achicsefvi.isltafat s hhisttamoar bllllad\n",
      "thedlrhedanf r8fhrecsg ar. hefecid shetLis asis \n",
      "\n",
      "Iteration 38000, Loss: 55.36721787930734\n",
      "Sample Text:\n",
      "itotomirrtha towotovori toerssmrarmmi mortrorrthtironm\n",
      "or ormmoo\n",
      "mmwoand ow\n",
      "dio, wtwot, anvow wonroomiya. womarronrad ou do' raw y aartttcond e worarora,as, ar taand  aarmra a, o da ot whfotore,orrror\n",
      "\n",
      "Iteration 39000, Loss: 63.046900555055316\n",
      "Sample Text:\n",
      "on msrwendud haneia lpMmefan farthsllsgnclal acusllmmad ol, aiddkdmwl\n",
      "fgnuncid' foscpI h'ad ffa,lmolflcawdchpis aya h\n",
      "en asnyas gig ounpa rsau'n od idul h\n",
      "an' win aa ald a''ems elald pcanl plh pllog i\n",
      "\n",
      "Iteration 40000, Loss: 67.48143636912532\n",
      "Sample Text:\n",
      "syonnt mod hhthh wekoshd\n",
      "on whung;,om hhany;s y; y oyoduuouud oson shsousot usos hso nto fsmo soy i;;ul sod\n",
      "thbhho;ul\n",
      "suun; etud thon\n",
      "d whBo uudeu , dhthsok,r. oounu ;oyhbh mhot . un euny bmhonnvhhego\n",
      "\n",
      "Iteration 41000, Loss: 66.02267106910965\n",
      "Sample Text:\n",
      " es o aspessd ece vaetand apse ladeppuspteeppher\n",
      "acde\n",
      "e ppi t. e, pcoftod.etrecesva ododelpbbpasacdes\n",
      "ha lapHeslevit i. Arg paln pend seiche cgepme imsblit thilpwals lid bcelwHbpen med pwa e sas. thel\n",
      "\n",
      "Iteration 42000, Loss: 67.73473071939097\n",
      "Sample Text:\n",
      " ed.n irsr\n",
      "tenrpi sed rer amter, rsmfem em\n",
      "e enn pmTeerr? e irNn ho\n",
      "ndeunr, e hrip ei errin pnomrend Te y thpepradun, at\n",
      "psu tense e Chhe thCtid y\"med\n",
      "ed, i nrTe pta mam e tee timless u . pan ent mera\n",
      "\n",
      "Iteration 43000, Loss: 64.96747796093844\n",
      "Sample Text:\n",
      "eg le e esviv bua\n",
      "o waw\n",
      "fhYa af e\n",
      " Sbe on \"e cag. uf e; ow ag e erw wef sacavos mo a wolwoe os e o oved o ava womo\n",
      "wiw o luvh\n",
      "wo\"gla oY an\n",
      " o ovhov avaovho o. houvheevho a nhe\n",
      "a vove fe\n",
      "unYpTha u bfh.\n",
      "\n",
      "Iteration 44000, Loss: 53.442261910121026\n",
      "Sample Text:\n",
      " if e! \n",
      "teci e ly Te alpeviTi  lae deped selrre Toin e the d lirs Tis! IvTw!! e inr! Thvhe !lTe we. \"ee if fTsh! s!nsgrTernen he!lel  en. se e hhsefhl hoswwhet! tiThlle uysle\"pree y ec! iro irterfren \n",
      "\n",
      "Iteration 45000, Loss: 51.87160064292677\n",
      "Sample Text:\n",
      " thha thont e thctewrrroet esd o ththtant\n",
      "y whi or thtet ar od r, thot taed at od ifat ed thona  dhce ad,ce tit\n",
      "lhi kitre nit Fht e tonnnd ocent an h tiast, o Va thos whtrhat o  a int tar ththwhan ton\n",
      "\n",
      "Iteration 46000, Loss: 56.07684339877146\n",
      "Sample Text:\n",
      "d eed emameg issd. eitebat ies Fbe.. FsermChCeo i arvtate then  e o Otaes rtetemem hs oe ares be ede Ceca sFo ted pe e aoi\n",
      "mtiti\n",
      "mt. e ose ees i ed.ses a a edtee isg,'tepa Fod empFad mtaespi thicmec. \n",
      "\n",
      "Iteration 47000, Loss: 64.50128979670195\n",
      "Sample Text:\n",
      "ontlTed-es\n",
      "am teun esmounte kre sthsit,stte etnsfturt osins te tAoew etrttoved\n",
      "ttols et  eet eAtte:seestsne ei ertTuvtfetors fo wtesvos o ftos offusfnTstee\n",
      "uest Ist of tes usgd Tot etorfs hnfrotuptte \n",
      "\n",
      "Iteration 48000, Loss: 56.89947794309596\n",
      "Sample Text:\n",
      "thndad cthdadend ent.nt,, engted andae thendsnthccbe cbthed stad,sroldedadand unnecdglcic ennd\n",
      "et the ad the ed coy ed e, bto octdRod ag inddvtedhthel tiMin gtbfiddthecbthechanls thmennn\n",
      "tadtfat cy en\n",
      "\n",
      "Iteration 49000, Loss: 66.55610887480643\n",
      "Sample Text:\n",
      "ivh. o\n",
      "re,ou rir ctijorecured .oru re ectituo ivyd y ptho fo be\n",
      "o reu the od bin ennvime; Yiburu be te it betid tlbe se ho o on brejova ig enusttotelt\n",
      "ir jasrrer eg oen\n",
      "ud e i ev fe heur. tes\n",
      "irlevhce\n",
      "\n",
      "Iteration 50000, Loss: 47.237021639782995\n",
      "Sample Text:\n",
      ")u ae I iy os thmhas Wwaa thcy y a A tetaIvhe re ustrhy o tias thi Iwy y rhtha e o thte y y pa ses ptetiyleusilfs where. af teby wi.ul a u e a sen rabe tero padhimy aw hithCcy Iw I, we, unly kos s wen\n",
      "\n",
      "Iteration 51000, Loss: 48.94791673215323\n",
      "Sample Text:\n",
      " this whw shciththe y omtmes che siimwctit i shess the nant-iw shesn cmwes wosth\n",
      " e a m whacy isethtiche ant tevas tethe het mtirhsessmes, sa, , mwa, thi dhache mhas wsyhthla, s thththan, heed, lthe, \n",
      "\n",
      "Iteration 52000, Loss: 51.003797708000896\n",
      "Sample Text:\n",
      ", or a, tie tanr e, pens fer azheer oeys\n",
      "ththcues\n",
      "enrsg tas s\n",
      "welssu, ba fa we\n",
      "ot asenthay a at the a nh rem afrer ar a sueans thers ree la on, lee e, wet\n",
      "at avires at tas at thre raf then,y thtased r\n",
      "\n",
      "Iteration 53000, Loss: 62.97536266187763\n",
      "Sample Text:\n",
      "ecilfw\n",
      "erreu od e o cho \n",
      "mi.e enrtyr\n",
      "a,e chi, seu. Fma ees i, O\n",
      "thwe a\n",
      "e e elce e o a. ecoilrmhrththecpen Thechle i weiryfbu\n",
      "ebe ud ered . enr leerpe. taerbwe haue macefh e pbenanssrtnupre ut lerff\n",
      "ce\n",
      "\n",
      "Iteration 54000, Loss: 59.288072853768625\n",
      "Sample Text:\n",
      ";d osdty, en o , lt tomrlosnsv ilcltt i rtol te id lay y elt e  he ron let klon the  cat tirtirral wi tped wrwrod y o rowrrortoril, dhrCththat,ol tonrl i El ro, te tuthte i o\n",
      "conns e rtid otre er nsa \n",
      "\n",
      "Iteration 55000, Loss: 66.06096907431139\n",
      "Sample Text:\n",
      "g ig\n",
      "ui ththe.tit lthot ut qlot i lold thhii tho y the thy ol o..l ostlul thbdhos odhlSl lopt it ulll tape uld lce, ig et  S yod il de.e e los g od billg som .hegdlged tho . toi to\n",
      "d Si the th MhdSle \n",
      "\n",
      "Iteration 56000, Loss: 71.46382806215605\n",
      "Sample Text:\n",
      "an\"\n",
      "fyrDnt\"D o wItiug a. wtwgant\" tii wo wonnty wiocwara onnvfenrtot\n",
      "a Datgut\n",
      "Dat t\".\n",
      "uu whHuit\n",
      "oDf\n",
      "\"the. o Da\n",
      "pvwiDcig,\n",
      "wecy ag, i\"nfnlw,\"aeni\"mt enng\n",
      "to bgipa \"i\n",
      "\"to enO\n",
      "than Dtoa oty\"nn, hgwoa\"g\"n \n",
      "\n",
      "Iteration 57000, Loss: 62.6528936037557\n",
      "Sample Text:\n",
      " ad wcet andrwhan hcthod totirdassd\n",
      "od\n",
      "lIi thacoddt ans a  hosjo . Acset,t hegechag at escthcarwacwoscro \n",
      "scbfatf bad ond\n",
      "ttacon thaf thsa rrcoenot oned ratacstod wtwanth a cthichad ha telons gia arde\n",
      "\n",
      "Iteration 58000, Loss: 59.978574515051996\n",
      "Sample Text:\n",
      "dot an o; anw a wa Tad woA ijwaa tot, tit;a jdra tea wiejaBjwwjwa wid wtas wwba twa;;ri ew Far\n",
      "wwawu want cpin hpan ry\n",
      "a,, RwaCAja\n",
      " wwod i taa E wad\n",
      "a ajhe;ajhtadttiaRmt.; ann; oi a wody whanrNt o ata\n",
      "\n",
      "Iteration 59000, Loss: 57.92901846711998\n",
      "Sample Text:\n",
      " hhtalLo thMwo ochashdée, Mat ad arfo 0csmeon,éin onnnthe, uLtad orhi hhped elgédéi, o BoLLLag Lod tho en as LiLour iud\n",
      "rhLod id hod thocho ronlt entéalaca o, sad dhot\n",
      "thpMu orfdhuh amit ann io Léoe e\n",
      "\n",
      "Iteration 60000, Loss: 51.323043906535766\n",
      "Sample Text:\n",
      "a tho wh-rhlan iltlHe e; thtoe thos o i go a  -lela il thas in e \"t fa fen nthfha lfenne tom gil ol e wethe thal al thi onrlneme lig i, o h as -pe. an whis a chwhes els lan tha elnn felslt  M nthtafis\n",
      "\n",
      "Iteration 61000, Loss: 66.36047499305518\n",
      "Sample Text:\n",
      "kslAfnrtre\n",
      "rcri innu la lalg s;lercnlad on,,rcaf ek, slca as connes ren anfrlar ki o in ccsockekcIw a fsecku\n",
      "larn a palkcuk whunrterlk ras lavannlnnnlsnnnnd c ha llag hrocanr af o rgascm; a a rhams lu\n",
      "\n",
      "Iteration 62000, Loss: 57.934166822570454\n",
      "Sample Text:\n",
      "ss thwies af onns, ereon, oer apoce\n",
      "seres ees alo ennnf, roro en es ire e e, enld e iaom opthocelsrpoe e is lees wafi ot theg oe inren te; a thet.eloe ed hame irami oor\n",
      "thhhons\n",
      "thmemonn at emipet that\n",
      "\n",
      "Iteration 63000, Loss: 66.02537945101858\n",
      "Sample Text:\n",
      "tvesg rseerervsee\n",
      "g et o ogig ceeng os wsengrat mot onlsm1o otu, eg aMrey't eptionnrs ont agyrenis een ey a winya wgel e o ad y gemineat et es tuorot enng eeggnleomiefsmtooges\n",
      "theg ans at en. entond o\n",
      "\n",
      "Iteration 64000, Loss: 61.79625859439636\n",
      "Sample Text:\n",
      "et\n",
      "e thhwit wholt rentysortft shestrfgarsrslw thonret e is asgetrwwerr uerlpaf i 1ghwof i lherens sher,ilf,ag the\n",
      "sslli iieg e\n",
      "ron sriryis lwes os rerreg au,rres thit hon\n",
      "essctos ot\n",
      "wisstllhifrissf it\n",
      "\n",
      "Iteration 65000, Loss: 51.88869423938837\n",
      "Sample Text:\n",
      " veceterero ththavyrnkhecthseucesoma the  sor eer-benmsrerprithlma e evtea fee arudha cethi tharpthenrela thssrire , eranpes\n",
      "fho ont icia as she,;s toaf tosiththcisesapnshre aptmeee chthceu, esrsefee \n",
      "\n",
      "Iteration 66000, Loss: 58.052138946360735\n",
      "Sample Text:\n",
      "cocra ond rad ad-Lsto efn lllba d titigteteld tecffan thirld ecanbar;;d erl blat ii raet e in er elt e ed tie.-lbonld;d erd thf,oti  thid lte llfatad hhra\n",
      "or ale net i du ated wulet ee a\n",
      "d a britTil-b\n",
      "\n",
      "Iteration 67000, Loss: 57.83958237773069\n",
      "Sample Text:\n",
      "fel enf a\n",
      "gheg wre tha e, an e ans owonvwro thhhe ose o\n",
      "e envha.\n",
      "efdn ef a eiul ar o a, ad e slens\n",
      "enesnd oses o esen o an egun,e whemHevoyeen ef efa oennthsons eflthaf thoggso as rey hci ho a the ete\n",
      "\n",
      "Iteration 68000, Loss: 55.95929537750207\n",
      "Sample Text:\n",
      "ens er ca en rpiuctop;\n",
      "bche insi e iP tisfde; capcpece ed a pecei  Mhi he pwopesu be  ceThink bhPapi' ice wiubans  wew en hen i e cteen ePnce en o inn eMpen ce i Feli e Inunl inilci hiefst petu; bisnt\n",
      "\n",
      "Iteration 69000, Loss: 61.89113701422747\n",
      "Sample Text:\n",
      " o emlel. of eolpe, echeicae e, i mepefi ocieemppa om olrot eremepef o ielfpwrnarolmee eefu, thelmmos ay eleetu is olimifloses peoveyeenn hanpe eeolape. ow eecallala toeu\n",
      "geceofserwe\n",
      "e lalfes ongil le\n",
      "\n",
      "Iteration 70000, Loss: 58.99359346757141\n",
      "Sample Text:\n",
      "l ed obecadu or eddn et .inco e utheepe udltheew et seeud tewed olcaren og ru,oes i a y u od ad ed fbe teToud\n",
      "enti benlsks ad a annd eu weog ef euco a bbow bed mwrulgebuwenld bo nirrwiledid\n",
      "thla ghog \n",
      "\n",
      "Iteration 71000, Loss: 76.45197553597637\n",
      "Sample Text:\n",
      " ed otibd o, oog be ed yhrttyt thumoe\n",
      "ot bbo togherb htarur e .-auad oco rhhyputhebg, yytsvedhs orlurg teuar aboeu te,enrmos ed hem\n",
      "bbthe neho y emy a a y y enri myk\n",
      "t odecsthbishnnr be\n",
      "bat\n",
      "o,rbthamre\n",
      "\n",
      "Iteration 72000, Loss: 68.78187263044804\n",
      "Sample Text:\n",
      "wober uestt Gacesrgafa, y a ot lbalraes y taunnnnaialgid ut o; ewtefes btota e et wabbo. arbrtetolntyt oiro fh o m; a toeunt it eeset Fghwannltafgi ge o nis a at 8ay\n",
      "issntene o at as arasdty o ortet r\n",
      "\n",
      "Iteration 73000, Loss: 68.91483603697709\n",
      "Sample Text:\n",
      "chhily ie ine iy s\n",
      "serhsTas,nchthaenle kee aonnc solshkenchey e-rlssee ke,\n",
      "thlirkefhthththes ilct   see, hinlen i ire efi by llthchlve. o  on foshsee ely Vhertl, saecasvHolos pkhka msy le ot y cia kki\n",
      "\n",
      "Iteration 74000, Loss: 64.5945402916638\n",
      "Sample Text:\n",
      "ug ou Thm\"mis rtherd niueurmed thes,:d sy\n",
      " ummri: stes \"duvsthu ins i onnmn a\n",
      "ner,us:n\n",
      "rur\n",
      "somkut.nnmtk, erurtusny thusut as thshthuu iid ituu temrthint e ttig onermsi mis rbet:, mrurcuumuons i, tok n\n",
      "\n",
      "Iteration 75000, Loss: 64.14377144318601\n",
      "Sample Text:\n",
      "em-\n",
      " Nhtha,h mot ed Bbke ocor ae, od as o eocar o ime-ama-s thoe Ent as:g:d aP-s oce::s IR es khos tototet: imtt\n",
      "ede: eik ot\n",
      "amre z:s an Terora sheo\n",
      "rt Dot\n",
      "wos ed.r:es een o- whCo thIntot anethinco mT\n",
      "\n",
      "Iteration 76000, Loss: 59.13661702356138\n",
      "Sample Text:\n",
      "thare refu heu rlerelt fa evher e eeeg isri kerrhtad e hterefensme et ie etevete hece es e\n",
      "det a e\n",
      "rer ee bucet hed ad Slee ad edet teelvud he nerreere er et ei leecdu ththar\n",
      "ue arrueded trhe en Se se\n",
      "\n",
      "Iteration 77000, Loss: 64.77266636008143\n",
      "Sample Text:\n",
      "\n",
      "dy pCetep eveis enm\n",
      "ren mvwetotek eonbet,vt les.etet ec\n",
      "odvtoot ot\n",
      "it e co d wod.\n",
      "mlp e\n",
      "ueCut oy et fet yibcold et ttce rit hirwdrolfr pad evisstoebd ieft ed\n",
      "ssHuteu t toimtu t emCdtry ursf ilodisit \n",
      "\n",
      "Iteration 78000, Loss: 55.98171531897429\n",
      "Sample Text:\n",
      "gs, ed e nbIhrg,\n",
      "elns lon ennlg w, gu, at Io if e ai thhomnne s stlliit:ler\n",
      "ncalt ith a tet eeg ala, himn min  innnw ad lrin e oennnngdg iwinegrenmtnit\n",
      "ti hi h\n",
      "aio, hree g, shthi en aftnnt e,,n ins se\n",
      "\n",
      "Iteration 79000, Loss: 58.610629309096915\n",
      "Sample Text:\n",
      "t ilg tabonnpitung Thgo arog . ant bo ong onun oset egst ot tovth bo at Taty inngrtobbnbin int tTig en bag\n",
      "ent agtt ol apta wi. tightoy ibot fat Ttit itinns, niug y obad thian o oy a taiKn bttont ot  \n",
      "\n",
      "Iteration 80000, Loss: 65.52184212814004\n",
      "Sample Text:\n",
      "hil nilt\n",
      "pesdt ons et\n",
      "wreins onetislmil lthgle ind,n in thrlpwad iilsdlnrrmil wogete ras iwanddnitrisnt tetinnlntet bimnsinl\n",
      "sllsri et Cinl tlatet omlwat ifss e,tpwly\n",
      "the lmrelu illi raeslresltlsae th\n",
      "\n",
      "Iteration 81000, Loss: 65.24945162551992\n",
      "Sample Text:\n",
      "and a at \n",
      "thamrerg\n",
      " G  fmado phlret emnnef e g welverrreusu a fta aer ha heamvut torenf the\n",
      "e emres rer teemnnmygmttremfrsRe e de enrma re femmfe thanthes go et od imemU e greut ow heren ogarg hag ef \n",
      "\n",
      "Iteration 82000, Loss: 61.407641515922535\n",
      "Sample Text:\n",
      "tichit aaret olvinridd, ab litit beenl stkes ariltef illkithet i i et e\n",
      "at li if at sos\n",
      "amog thJoif woltheted\n",
      "e, et iribat it is itirhiri woftestlterhsiot ia elrieily re. i fhthidif\n",
      "Mtsi\n",
      "ih at sohipth\n",
      "\n",
      "Iteration 83000, Loss: 59.25111603873075\n",
      "Sample Text:\n",
      "hcung ersnmtghad ed e iCnrjetpne hTphrhau  uo e pthici comyr or\n",
      "mourer.rep thre of pe\n",
      "phHau conrdsnprrkphu, derrecpmrTenu\n",
      "pgefsr. opppTheppMDThochmaun prerrwecer Therd\n",
      "pwer e pre ThwphThmgieu Cue.\n",
      "o u\n",
      "\n",
      "Iteration 84000, Loss: 61.551552244137426\n",
      "Sample Text:\n",
      "n a exisenvisttalrtixwand erns mo thad aunmfus\n",
      "olmagi raf angnlte, bi banierdyg oeumtthos afe et ae ho ilidhnTkeudeis\"lrtail e e s, a ed ad et ekn ot tiannn af a a,sniere e thweid wa go o camopinns ap\n",
      "\n",
      "Iteration 85000, Loss: 63.956862075103594\n",
      "Sample Text:\n",
      "c.nvin  a aw Thwe ad ang,tift ethupalctandennndei at thwhenwed iet erd/adton on at og ond ad enariiaud a, o paraseed dhnobaensd bbu tha ad whak idena was aad, it inu itrot eedo hannn iw, efe sinthmto \n",
      "\n",
      "Iteration 86000, Loss: 60.88856504064582\n",
      "Sample Text:\n",
      "rfsrwenf oacTsonnme maonrr oon anrc esepnnrceroco end ap ara sfmoublssfYaet arsnses astf bawondef eng af as es o lenbepdi enmpes ermamB a e wmbsen imonpwlbod tassludellnnrnnnscanrbed okorsmwsbmind rwe\n",
      "\n",
      "Iteration 87000, Loss: 54.41688620238713\n",
      "Sample Text:\n",
      "onrte. lphai peemieretupnprerkeyklarenr a cenlipi ismphky ,\n",
      "od\n",
      "ires,tinlpvippis eropat,pelyitai e iyeefnle pyenssndeniurtitenliyiryd\n",
      "y erepias thpadikanspaw.r octurppistas iriid elthwerid\n",
      "ekovecisape,\n",
      "\n",
      "Iteration 88000, Loss: 62.218948107747174\n",
      "Sample Text:\n",
      " shonanennfhthe Soiten els enin ad thSa. toos bae Sos'nanmthSNenersStho\"f Sas o wyth'a iot fay.nte enmontoa\n",
      "lao\n",
      "anfje isi aly reSeat y enon, ishae\n",
      "shshad.ni iwonNas.o o shGamolansirtod sh\"\"e,sis Sonot\n",
      "\n",
      "Iteration 89000, Loss: 55.54648773822623\n",
      "Sample Text:\n",
      "ci, eg e d pond bdines lwonn ec o cofkd,n my ulnndidh eiwnn, sasgng ensen allfsid na id acsucnmhsi chen loy cint ched bus pancglca csfes ous ennt sa, ig, ilrcy wad in gichdlgeg, ,,,es odhd dhsolg cecy\n",
      "\n",
      "Iteration 90000, Loss: 57.785495997326635\n",
      "Sample Text:\n",
      "rva l  nthilccfenld  ollmht bas le alpclrSlot eil allt lald bo al lef,,t mupcsa mat aemftal bans, afe\n",
      "rplganlpcbppale a\n",
      "s tamfat epndil, sll a sa, elt alnle ot borenmpar\n",
      "tas y ag al lta, o rose kma th\n",
      "\n",
      "Iteration 91000, Loss: 58.99738225112056\n",
      "Sample Text:\n",
      "unxthenf\n",
      "lwon imeu octthonend our- o anvan o o o ed inxi ha acannchbat tha alne a at\n",
      "ane co celchectel e axThmeem i  thant ochhe ann-donnnnenltenn ac.r qo i e\n",
      "ha annt he i elllrfni ii e an a at alcaxp\n",
      "\n",
      "Iteration 92000, Loss: 54.613264499833306\n",
      "Sample Text:\n",
      "herrro, htoininrte,,g,pthe, pe thiy hhen, hhhIyrpovhe ro ghthi i hedenperrhhCas dhr pho  or ht ho\n",
      "ore thha', iy ont\n",
      "thhhtho dhchtho hinin o euy an,etsth, ,hotheth, honngerfh ThrhthhhthSet e\n",
      "y en rhid \n",
      "\n",
      "Iteration 93000, Loss: 60.44296141459566\n",
      "Sample Text:\n",
      "ae od tha Ne re\n",
      "had thanns taGog\n",
      "enunutund Sn e\n",
      "Ranrteid lmonrat Vartliththanftat od thJad\n",
      "thbuit\n",
      "thiththad\n",
      "e. uncannu ay arhid,t res thtatnuinnleed at,nat emthaug icecie shureor thbaf\n",
      "dhMnththVau edu\n",
      "\n",
      "Iteration 94000, Loss: 60.01022512843485\n",
      "Sample Text:\n",
      "disrfdarwd ad ald eddc  a acoid smad Hmalbmal laceg adtd abrat afrablserd o gdlsolllnafd o aPn bag uug a ennccost oca us bad saburacdcod ove at thcba asllisdm'fi nallu Nlwa anns o o ascbas\n",
      "aln y adll,\n",
      "\n",
      "Iteration 95000, Loss: 66.17599217432615\n",
      "Sample Text:\n",
      "dn,ho arod tiresf allborsnroelni thhirranerd e odsd efdrpol dia \n",
      "eslnerssoteed,asrthodet\n",
      "e av,\n",
      "lagoovas, prorterlthrensntod ra,lorst e to, btasevauncadet, thinfrshad rotin the. ossnri ed sevedrori ou \n",
      "\n",
      "Iteration 96000, Loss: 60.81190911859369\n",
      "Sample Text:\n",
      "n cE\n",
      "fen\n",
      "Ent, iujhbLsang. ansus Ed nEes aojod oen mEf e,onn Es tose ot soochbsosensnf ount erfog yos fhos ThtoronjonmPan,,ny o thcos ood af,jhovey i efnomoy oLks Hfonfro Ew Erk oInft fof\n",
      "LoLison bonos\n",
      "\n",
      "Iteration 97000, Loss: 69.64036469573318\n",
      "Sample Text:\n",
      "o y o Re arucog horrthiug lRos los alssfles i Roc\n",
      "keu gy thHas eG as as od bs ilo g tholpththlesscun tis linnt shpat ses WThas e bRu wee lyeus ey\n",
      "pslthosev.rtt a tes ltheny wIsethlturor.ctcoryt hof fo\n",
      "\n",
      "Iteration 98000, Loss: 67.42874996351328\n",
      "Sample Text:\n",
      " bmrdathlGGatio tarurubu sora borad a enuta soa odid o Gba ud ualus lGsubepnarora3dartThGkuogtpapulfilrbNhltoar lbswitsta\n",
      "la ivGuNathuba teublos a ierNama lpfpoud\n",
      "hNalopant bnrtethsereuy atictushoen e\n",
      "\n",
      "Iteration 99000, Loss: 56.490740794884246\n",
      "Sample Text:\n",
      "çt i hd ir fet bar on thot tae. e icerlroci ad\n",
      "ef an bfe evor a e a aeris chbli rbor ar htamlazer\n",
      "of a bp ay orrmen be red thsi thert o e  e pIs e thop. o e ras es e eceto tho en ot mwad\n",
      "parud emror a\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the RNN\n",
    "hidden_size = 100  # You can adjust this\n",
    "rnn = RNN(vocab_size, hidden_size)\n",
    "\n",
    "# Train the RNN\n",
    "train_rnn(rnn, text, char_to_ix, ix_to_char, seq_length=25, num_iterations=100_000, print_every=1_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Study of the IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
